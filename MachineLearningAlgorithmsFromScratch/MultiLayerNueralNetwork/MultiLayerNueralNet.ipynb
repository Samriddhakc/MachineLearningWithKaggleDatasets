{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from HelperFunctions.ipynb\n",
      "Sigmoid at 0 is 0.5\n",
      "relu at 0 is 0\n",
      "relu at -ve is 0\n",
      "relu at +ve is [[0 0 1 2]\n",
      " [0 0 0 0]]\n",
      "1.8986161189963753\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "import sklearn.linear_model\n",
    "import sklearn.linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "import import_ipynb\n",
    "from HelperFunctions import relu,sigmoid,loss\n",
    "np.random.seed(1) # set a seed so that the results are consistent\n",
    "#from ipynb.fs.full.HelperFunctions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''from sklearn.datasets.samples_generator import make_blobs\n",
    "from matplotlib import pyplot\n",
    "from pandas import DataFrame\n",
    "# generate 2d classification dataset\n",
    "X, Y = make_blobs(n_samples=200, centers=2, n_features=2)\n",
    "# scatter plot, dots colored by class value\n",
    "df = DataFrame(dict(x=X[:,0], y=X[:,1], label=Y))\n",
    "colors = {0:'red', 1:'blue', 2:'green'}\n",
    "fig, ax = pyplot.subplots()\n",
    "grouped = df.groupby('label')\n",
    "for key, group in grouped:\n",
    "    group.plot(ax=ax, kind='scatter', x='x', y='y', label=key, color=colors[key])\n",
    "pyplot.show()'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#data_to_test the algorithm\n",
    "from sklearn.datasets import load_iris\n",
    "data=load_iris()\n",
    "X=data.data\n",
    "Y=data.target\n",
    "Y[Y>0]=1\n",
    "feature_mean=np.mean(X,axis=0)\n",
    "feature_SD=np.std(X,axis=0)\n",
    "X=(X-feature_mean)/feature_SD\n",
    "X\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(100, 4)\n",
      "(50, 4)\n",
      "(100,)\n",
      "(50,)\n"
     ]
    }
   ],
   "source": [
    "Y.reshape((Y.shape[0],1))\n",
    "x_train,x_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, random_state=42)\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(4, 100)\n",
      "(4, 50)\n",
      "(1, 100)\n",
      "(1, 50)\n"
     ]
    }
   ],
   "source": [
    "x_train=x_train.T\n",
    "x_test=x_test.T\n",
    "y_train=y_train.reshape((y_train.shape[0],1))\n",
    "y_train=y_train.T\n",
    "y_test=y_test.reshape((y_test.shape[0],1))\n",
    "y_test=y_test.T\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Intialize Model Parameters\n",
    "\n",
    "class MultiLayerNueralNet():\n",
    "    \n",
    "    \n",
    "    def __init__(self,num_input,num_hidden,num_output):\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        intialize class  instance variables \n",
    "        Argument:\n",
    "        num_input -- size of the input layer\n",
    "        num_hidden -- size of the hidden layer\n",
    "        num_ouput -- size of the output layer\n",
    "        \n",
    "        \"\"\"\n",
    "        np.random.seed(2)\n",
    "        self.num_input=num_input\n",
    "        self.num_hidden=num_hidden\n",
    "        self.num_output=num_output\n",
    "        self.W1=np.random.rand(num_hidden,num_input)\n",
    "        self.W2=np.random.rand(num_output,num_hidden)*0.01\n",
    "        self.b1=np.zeros((num_hidden,1))\n",
    "        self.b2=np.zeros((num_output,1))\n",
    "        self.cache={\"Z1\": None,\"A1\": None,\"Z2\": None,\"A2\": None}\n",
    "        self.grad_cache={\"dW1\": None,\"dW2\": None,\"db1\": None,\"db2\": None}\n",
    "        \n",
    "   \n",
    "    \n",
    "    def forward_propgation(self,X):\n",
    "        \n",
    "        W1=self.W1\n",
    "        W2=self.W2\n",
    "        b1=self.b1\n",
    "        b2=self.b2\n",
    "        Z1=np.dot(W1,X)+b1\n",
    "        A1=sigmoid(Z1)\n",
    "        Z2=np.dot(W2,A1)+b2\n",
    "        A2=sigmoid(Z2)\n",
    "        self.cache[\"Z1\"]=Z1\n",
    "        self.cache[\"A1\"]=A1\n",
    "        self.cache[\"Z2\"]=Z2\n",
    "        self.cache[\"A2\"]=A2\n",
    "        return A2\n",
    "    \n",
    "    def backward_propagation(self,X,Y):\n",
    "    \n",
    "        #one_hot_y=np.eye(3)[Y]\n",
    "        m=X.shape[1]\n",
    "        W1=self.W1\n",
    "        W2=self.W2\n",
    "        b1=self.b1\n",
    "        b2=self.b2\n",
    "        Z1=self.cache[\"Z1\"]\n",
    "        A1=self.cache[\"A1\"]\n",
    "        Z2=self.cache[\"Z2\"]\n",
    "        A2=self.cache[\"A2\"]\n",
    "    \n",
    "        dZ2=A2 - Y\n",
    "        dW2=(1./m)*np.dot(dZ2,A1.T)\n",
    "        db2=(1./m)*np.sum(dZ2,axis=1, keepdims = True)\n",
    "        #dZ1=np.dot(W2.T,dZ2)*A1*(1-A1)\n",
    "        dZ1=np.multiply(np.dot(W2.T,dZ2),np.int64(A1 > 0))\n",
    "        dW1=(1./m)*np.dot(dZ1,X.T)\n",
    "        db1=(1./m)*np.sum(dZ1,axis=1, keepdims = True)\n",
    "        self.grad_cache[\"dW1\"]=dW1\n",
    "        self.grad_cache[\"dW2\"]=dW2\n",
    "        self.grad_cache[\"db1\"]=db1\n",
    "        self.grad_cache[\"db2\"]=db2\n",
    "        \n",
    "    #def gradient_check():\n",
    "        \n",
    "    \n",
    "    \n",
    "    def train(self,x_train,y_train,x_test,y_test,num_iterations,learning_rate=10):\n",
    "        \n",
    "        train_loss=[]\n",
    "        test_loss=[]\n",
    "        m_train=y_train.shape[1]\n",
    "        m_test=y_test.shape[1]\n",
    "        #one_hot_y=np.eye(3)[y_train]\n",
    "        for i in range(num_iterations):\n",
    "            \n",
    "            self.forward_propgation(x_train)\n",
    "            self.backward_propagation(x_train,y_train)\n",
    "            self.W1=self.W1-learning_rate*self.grad_cache[\"dW1\"]\n",
    "            print(self.W1)\n",
    "            self.W2=self.W2-learning_rate*self.grad_cache[\"dW2\"]\n",
    "            self.b1=self.b1-learning_rate*self.grad_cache[\"db1\"]\n",
    "            self.b2=self.b2-learning_rate*self.grad_cache[\"db2\"]\n",
    "            print\n",
    "            #this.optimized_cache[\"b1\"]=this.optimized_cache[\"b1\"]-learning_rate*this.grad_cache[\"db1\"]\n",
    "            #this.optimized_cache[\"b2\"]=this.optimized_cache[\"b2\"]-learning_rate*this.grad_cache[\"db2\"]    \n",
    "               \n",
    "            #print(self.cache[\"A2\"])\n",
    "            #print(one_hot_y)\n",
    "            if((i%10)==0):\n",
    "                \n",
    "                print(\"The train loss is\",loss(m_train,self.cache[\"A2\"],y_train))\n",
    "                train_loss.append(loss(m_train,self.cache[\"A2\"],y_train))\n",
    "                self.forward_propgation(x_test)\n",
    "                print(\"The val loss is\",loss(m_test,self.cache[\"A2\"],y_test))\n",
    "                test_loss.append(loss(m_test,self.cache[\"A2\"],y_test))\n",
    "        return train_loss,test_loss\n",
    "    \n",
    "    def predict(self,x):\n",
    "        \n",
    "        y_pred=self.forward_propgation(x_test)[0][0]\n",
    "        if (y_pred>=0.5):\n",
    "                return 1\n",
    "        else: \n",
    "            return 0\n",
    "        \n",
    "    def accuracy(self,x,y):\n",
    "        y_pred=self.forward_propgation(x)\n",
    "        y_pred[y_pred>=0.5]=1\n",
    "        y_pred[y_pred<0.5]=0\n",
    "        return (np.sum(y_pred==y)/y.size)*100\n",
    "        \n",
    "      \n",
    "    \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5166319448226169\n",
      "0.5166319448226169\n"
     ]
    }
   ],
   "source": [
    "nueralnet=MultiLayerNueralNet(x_train.shape[0],1,1)\n",
    "A2=nueralnet.forward_propgation(x_train)\n",
    "print(loss(134,A2,y_train))\n",
    "logprobs = np.multiply(-np.log(A2),y_train) + np.multiply(-np.log(1 - A2), 1 - y_train)\n",
    "cost = 1./134 * np.sum(logprobs)\n",
    "print(cost)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[0.4491395  0.01408802 0.56733023 0.45216673]]\n",
      "The train loss is 0.6922868060623067\n",
      "The val loss is 0.9032810428940357\n",
      "[[ 6.62639946 -4.5781701   8.28203879  7.84826676]]\n",
      "[[ 9.4020211  -8.5010458  12.74101441 12.03152649]]\n",
      "[[ 15.90026325 -13.91378806  21.1129068   20.07907203]]\n",
      "[[ 18.57794506 -18.46927121  25.76343989  24.50832192]]\n",
      "[[ 22.38332867 -23.28256862  31.45260411  29.95775214]]\n",
      "[[ 25.21190604 -27.83467661  36.17534647  34.47027752]]\n",
      "[[ 27.9653265  -32.31289398  40.80808521  38.94116105]]\n",
      "[[ 30.42957197 -36.70406425  45.10248405  43.11360042]]\n",
      "[[ 32.85782642 -40.73116837  49.1654382   47.09233431]]\n",
      "[[ 35.15228261 -44.39648185  52.93628987  50.80731373]]\n",
      "The train loss is 0.11206681954717963\n",
      "The val loss is 0.11165192595339864\n",
      "[[ 37.21075104 -47.93728977  56.42198982  54.26001981]]\n",
      "[[ 39.08809001 -51.41636694  59.69916136  57.52272461]]\n",
      "[[ 40.81460047 -54.85662222  62.81048936  60.63421442]]\n",
      "[[ 42.41365693 -58.27004569  65.78656275  63.62221222]]\n",
      "[[ 43.90383216 -61.66425277  68.65005919  66.50739709]]\n",
      "[[ 45.30043624 -65.04365778  71.41815371  69.3055349 ]]\n",
      "[[ 46.6201101  -68.40518553  74.10490399  72.02966907]]\n",
      "[[ 47.91528832 -71.69236919  76.72919948  74.69807426]]\n",
      "[[ 49.47494603 -74.51502581  79.35324381  77.37361888]]\n",
      "[[ 51.0334236  -77.10261324  81.85605326  79.93826469]]\n",
      "The train loss is 0.06434164909694326\n",
      "The val loss is 0.04357012690290645\n",
      "[[ 52.51103958 -79.57494263  84.2268638   82.38043779]]\n",
      "[[ 53.92363191 -81.95344696  86.49045133  84.72359552]]\n",
      "[[ 55.27428088 -84.26226901  88.6616801   86.98163945]]\n",
      "[[ 56.56612351 -86.51970926  90.75255612  89.16578769]]\n",
      "[[ 57.80289911 -88.73897287  92.77307801  91.28538717]]\n",
      "[[ 58.988852   -90.92913502  94.73163246  93.34828267]]\n",
      "[[ 60.12832992 -93.09628731  96.63521243  95.36101168]]\n",
      "[[ 61.2254507  -95.24452751  98.48961948  97.32898376]]\n",
      "[[ 62.28396281 -97.37662902 100.2996816   99.25667403]]\n",
      "[[ 63.30726335 -99.49440663 102.06946807 101.14780705]]\n",
      "The train loss is 0.05322260031450189\n",
      "The val loss is 0.028114246561233215\n",
      "[[  64.29853849 -101.59880613  103.80249581  103.00551299]]\n",
      "[[  65.26107196 -103.68963893  105.50195762  104.83245424]]\n",
      "[[  66.19893538 -105.76462175  107.17106939  106.63093522]]\n",
      "[[  67.11872109 -107.81666941  108.81381782  108.40303667]]\n",
      "[[  68.03406283 -109.82663311  110.43683438  110.15087127]]\n",
      "[[  68.97314837 -111.75035776  112.05220857  111.87658368]]\n",
      "[[  69.95904329 -113.54237832  113.66659034  113.57738925]]\n",
      "[[  70.94928855 -115.24470623  115.25329041  115.23814494]]\n",
      "[[  71.91284232 -116.89806544  116.79628358  116.85264883]]\n",
      "[[  72.85441117 -118.50191567  118.29998061  118.42458383]]\n",
      "The train loss is 0.027501113600965094\n",
      "The val loss is 0.02091241520398782\n",
      "[[  73.77350763 -120.06143506  119.76572867  119.9560621 ]]\n",
      "[[  74.67147235 -121.57914791  121.19573159  121.44943692]]\n",
      "[[  75.5489541  -123.05804131  122.59168122  122.90670691]]\n",
      "[[  76.40692096 -124.50035949  123.95532175  124.32977695]]\n",
      "[[  77.23280062 -125.82400256  125.24522711  125.67819832]]\n",
      "[[  78.02572582 -127.13887331  126.49946229  126.99445686]]\n",
      "[[  78.81251218 -128.41268124  127.73291126  128.28534959]]\n",
      "[[  79.57674558 -129.66874865  128.93773356  129.54846395]]\n",
      "[[  80.33056936 -130.8932643   130.12096174  130.78727659]]\n",
      "[[  81.06650688 -132.09781446  131.27936843  132.00116181]]\n",
      "The train loss is 0.01961345542992143\n",
      "The val loss is 0.015606316967410976\n",
      "[[  81.79087194 -133.2758718   132.41684522  133.19224844]]\n",
      "[[  82.49996745 -134.43391416  133.53206411  134.36059493]]\n",
      "[[  83.19742438 -135.56870105  134.62744755  135.50767776]]\n",
      "[[  83.88134343 -136.6841852   135.70253719  136.63383402]]\n",
      "[[  84.55397667 -137.77875962  136.75898026  137.74017742]]\n",
      "[[  85.21435491 -138.85500245  137.796744    138.82715571]]\n",
      "[[  85.86262379 -139.85698351  138.78004743  139.86220064]]\n",
      "[[  86.48959532 -140.85695433  139.74184277  140.87834548]]\n",
      "[[  87.11486363 -141.82970981  140.69241573  141.87975551]]\n",
      "[[  87.72340007 -142.79627914  141.6244752   142.86406547]]\n",
      "The train loss is 0.015372340488754974\n",
      "The val loss is 0.01241394384160458\n",
      "[[  88.32779575 -143.74068373  142.54470853  143.8340586 ]]\n",
      "[[  88.89733484 -144.63636099  143.41974974  144.75744537]]\n",
      "[[  89.46483375 -145.50944022  144.28478573  145.66797607]]\n",
      "[[  89.99238839 -146.34812718  145.10355419  146.53085013]]\n",
      "[[  90.52765597 -147.15261725  145.91772595  147.38336919]]\n",
      "[[  91.04294115 -147.96120005  146.71367744  148.2209344 ]]\n",
      "[[  91.55984544 -148.74531577  147.50249888  149.04777803]]\n",
      "[[  92.0623814  -149.52735446  148.27642749  149.86138729]]\n",
      "[[  92.56299231 -150.29107772  149.041928    150.66430546]]\n",
      "[[  93.0527451  -151.0492561   149.79475522  151.45525712]]\n",
      "The train loss is 0.012375818731349542\n",
      "The val loss is 0.010248391662997151\n",
      "[[  93.53862096 -151.7929953   150.53861335  152.23577598]]\n",
      "[[  94.01582563 -152.52939331  151.27131087  153.00529829]]\n",
      "[[  94.47740056 -153.2148967   151.97148779  153.74366445]]\n",
      "[[  94.91581698 -153.86773686  152.63506147  154.44562158]]\n",
      "[[  95.35484841 -154.50336807  153.29311702  155.13950844]]\n",
      "[[  95.78383067 -155.13685998  153.94052371  155.82371304]]\n",
      "[[  96.21046947 -155.75807228  154.58125494  156.4997743 ]]\n",
      "[[  96.62968579 -156.37438687  155.21293976  157.16702679]]\n",
      "[[  97.04518257 -156.98111509  155.83755315  157.82630207]]\n",
      "[[  97.42162883 -157.52324835  156.41117529  158.43878689]]\n",
      "The train loss is 0.010385147881560844\n",
      "The val loss is 0.008698732602157904\n",
      "[[  97.80001073 -158.04971168  156.98100866  159.04487508]]\n",
      "[[  98.16977512 -158.57542408  157.54197277  159.64296686]]\n",
      "[[  98.53776516 -159.09125711  158.0975607   160.23438247]]\n",
      "[[  98.89990461 -159.60315899  158.64587761  160.8186002 ]]\n",
      "[[  99.25886074 -160.10774323  159.18834752  161.39622348]]\n",
      "[[  99.6131844  -160.60730366  159.72436205  161.96715966]]\n",
      "[[  99.96389263 -161.10071171  160.25452346  162.53172308]]\n",
      "[[ 100.28538077 -161.57568326  160.75632898  163.07020971]]\n",
      "[[ 100.58427412 -161.99371874  161.21348967  163.56335218]]\n",
      "[[ 100.88843754 -162.39452602  161.66956323  164.05205088]]\n",
      "The train loss is 0.008589978646321364\n",
      "The val loss is 0.007518843468119605\n",
      "[[ 101.1855913  -162.79547276  162.11863622  164.53444195]]\n",
      "[[ 101.48109032 -163.18937293  162.5634299   165.0115489 ]]\n",
      "[[ 101.7728491  -163.57938314  163.00301619  165.48316755]]\n",
      "[[ 102.06180413 -163.96444109  163.43794258  165.94958237]]\n",
      "[[ 102.34769074 -164.34514598  163.86816446  166.41087513]]\n",
      "[[ 102.63070155 -164.72144984  164.29385728  166.86719672]]\n",
      "[[ 102.91085537 -165.09353722  164.71510794  167.31866435]]\n",
      "[[ 103.18823055 -165.46150067  165.13202916  167.76540004]]\n",
      "[[ 103.46288158 -165.82545763  165.54471892  168.20751725]]\n",
      "[[ 103.73486821 -166.18551026  165.95327488  168.645126  ]]\n",
      "The train loss is 0.00748805513369763\n",
      "The val loss is 0.006579098150041651\n",
      "[[ 103.99133004 -166.52714695  166.33857102  169.06135568]]\n",
      "[[ 104.25072254 -166.85760876  166.72267677  169.47423834]]\n",
      "[[ 104.50624512 -167.1865631   167.10233573  169.88276717]]\n",
      "[[ 104.75975477 -167.51157663  167.47851285  170.28733001]]\n",
      "[[ 105.01085548 -167.83335698  167.85107161  170.68794728]]\n",
      "[[ 105.25969895 -168.15184045  168.22014048  171.08472445]]\n",
      "[[ 105.50630595 -168.46714071  168.58578199  171.47774271]]\n",
      "[[ 105.750725   -168.77932726  168.94807028  171.86708478]]\n",
      "[[ 105.99299697 -169.08847523  169.30707391  172.25282935]]\n",
      "[[ 106.21981088 -169.38434815  169.64609047  172.61786438]]\n",
      "The train loss is 0.006569492611870061\n",
      "The val loss is 0.0058451597900644236\n",
      "[[ 106.45049562 -169.66914398  169.98489331  172.98045935]]\n",
      "[[ 106.67837767 -169.95233244  170.32028056  173.33961645]]\n",
      "[[ 106.88072355 -170.23432263  170.63361255  173.67720219]]\n",
      "[[ 107.09303963 -170.49707626  170.94980717  174.01348942]]\n",
      "[[ 107.30308811 -170.7578837   171.26296152  174.34660382]]\n",
      "[[ 107.51145135 -171.01604566  171.5734028   174.67670414]]\n",
      "[[ 107.71816384 -171.27162614  171.88118992  175.00385901]]\n",
      "[[ 107.92325475 -171.52469201  172.18637748  175.32813361]]\n",
      "[[ 108.12675229 -171.7753072   172.48901804  175.64959048]]\n",
      "[[ 108.30312457 -172.00490695  172.76154205  175.93722851]]\n",
      "The train loss is 0.005680336570338182\n",
      "The val loss is 0.005234875189304224\n",
      "[[ 108.48637931 -172.22071244  173.03579621  176.22365445]]\n",
      "[[ 108.66910592 -172.43325594  173.3082264   176.50776498]]\n",
      "[[ 108.82447062 -172.63075469  173.54785433  176.75938088]]\n",
      "[[ 108.98745596 -172.8140494   173.78980209  177.01022091]]\n",
      "[[ 109.15114902 -172.99299878  174.03076652  177.25927254]]\n",
      "[[ 109.31427767 -173.16947528  174.27016293  177.50638402]]\n",
      "[[ 109.46131826 -173.3404031   174.49253449  177.7371144 ]]\n",
      "[[ 109.61188723 -173.5034255   174.71543296  177.96670526]]\n",
      "[[ 109.76301172 -173.66282257  174.93745357  178.19474387]]\n",
      "[[ 109.91380131 -173.81991619  175.15819193  178.42113119]]\n",
      "The train loss is 0.004812848343776688\n",
      "The val loss is 0.004711828313993432\n",
      "[[ 110.06394391 -173.97520497  175.37752076  178.64585435]]\n",
      "[[ 110.2133253  -174.12890434  175.59540695  178.86892897]]\n",
      "[[ 110.36190496 -174.28112282  175.81185194  179.09038047]]\n",
      "[[ 110.50967112 -174.43192607  176.0268701   179.31023726]]\n",
      "[[ 110.65662347 -174.5813615   176.24048053  179.52852827]]\n",
      "[[ 110.80276637 -174.72946811  176.45270382  179.74528195]]\n",
      "[[ 110.94810606 -174.8762805   176.66356076  179.96052598]]\n",
      "[[ 111.09264954 -175.02183067  176.87307186  180.1742871 ]]\n",
      "[[ 111.23640405 -175.16614874  177.08125712  180.38659115]]\n",
      "[[ 111.36953337 -175.30388223  177.27522553  180.58479133]]\n",
      "The train loss is 0.004352478039292869\n",
      "The val loss is 0.004286855921170255\n",
      "[[ 111.50413837 -175.43740672  177.46905552  180.78200987]]\n",
      "[[ 111.63909129 -175.56836534  177.66222196  180.97810098]]\n",
      "[[ 111.7738875  -175.6975109   177.85449707  181.17301021]]\n",
      "[[ 111.90828308 -175.82522309  178.04577713  181.36672097]]\n",
      "[[ 112.0421557  -175.95170697  178.2360161   181.55923396]]\n",
      "[[ 112.17544329 -176.07708056  178.42519631  181.75055815]]\n",
      "[[ 112.30811477 -176.20141672  178.61331451  181.94070652]]\n",
      "[[ 112.4401554  -176.32476411  178.80037488  182.12969385]]\n",
      "[[ 112.5715592  -176.44715805  178.98638549  182.31753571]]\n",
      "[[ 112.70232499 -176.5686263   179.17135632  182.50424785]]\n",
      "The train loss is 0.003940897680458453\n",
      "The val loss is 0.0039381972292758674\n",
      "[[ 112.83245418 -176.68919211  179.35529835  182.68984594]]\n",
      "[[ 112.96194966 -176.80887598  179.53822295  182.8743454 ]]\n",
      "[[ 113.09081512 -176.9276966   179.7201416   183.05776136]]\n",
      "[[ 113.21905472 -177.04567137  179.90106577  183.24010859]]\n",
      "[[ 113.34667287 -177.16281676  180.08100678  183.4214015 ]]\n",
      "[[ 113.47367411 -177.27914846  180.25997581  183.60165417]]\n",
      "[[ 113.60006307 -177.39468153  180.43798382  183.78088033]]\n",
      "[[ 113.72584439 -177.50943047  180.61504161  183.95909336]]\n",
      "[[ 113.85102275 -177.62340926  180.79115977  184.13630635]]\n",
      "[[ 113.97560282 -177.73663142  180.9663487   184.31253207]]\n",
      "The train loss is 0.0036399384612630327\n",
      "The val loss is 0.0036486774768138587\n",
      "[[ 114.09958926 -177.84911002  181.14061859  184.48778298]]\n",
      "[[ 114.2229867  -177.96085773  181.31397948  184.66207126]]\n",
      "[[ 114.34579977 -178.07188683  181.48644119  184.83540884]]\n",
      "[[ 114.46803309 -178.18220922  181.65801341  185.00780735]]\n",
      "[[ 114.58969124 -178.29183645  181.82870562  185.17927818]]\n",
      "[[ 114.71077878 -178.40077974  181.99852717  185.34983249]]\n",
      "[[ 114.83130024 -178.50904998  182.16748722  185.5194812 ]]\n",
      "[[ 114.95126013 -178.61665774  182.3355948   185.68823498]]\n",
      "[[ 115.07066295 -178.72361332  182.50285879  185.85610431]]\n",
      "[[ 115.18951315 -178.82992671  182.66928791  186.02309944]]\n",
      "The train loss is 0.0033870288878057263\n",
      "The val loss is 0.003402974302984995\n",
      "[[ 115.30781516 -178.93560764  182.83489074  186.18923043]]\n",
      "[[ 115.42557339 -179.04066558  182.99967574  186.35450713]]\n",
      "[[ 115.54279221 -179.14510975  183.16365122  186.51893921]]\n",
      "[[ 115.65183797 -179.24649432  183.31595792  186.67270363]]\n",
      "[[ 115.76164712 -179.34551979  183.46813386  186.82588266]]\n",
      "[[ 115.87179084 -179.44281727  183.61998187  186.97842366]]\n",
      "[[ 115.98200726 -179.53877737  183.77138353  187.13029742]]\n",
      "[[ 116.09212937 -179.6336538   183.92226514  187.28148799]]\n",
      "[[ 116.20204752 -179.72761733  184.07257995  187.43198734]]\n",
      "[[ 116.31168837 -179.82078616  184.22229823  187.58179241]]\n",
      "The train loss is 0.0031358285658153956\n",
      "The val loss is 0.003189478053940155\n",
      "[[ 116.42100229 -179.91324405  184.37140132  187.73090332]]\n",
      "[[ 116.52995553 -180.00505163  184.51987797  187.87932228]]\n",
      "[[ 116.63852513 -180.09625371  184.66772192  188.02705288]]\n",
      "[[ 116.74669562 -180.18688411  184.81493037  188.17409964]]\n",
      "[[ 116.85445677 -180.27696891  184.96150288  188.32046769]]\n",
      "[[ 116.96180199 -180.36652868  185.10744069  188.46616253]]\n",
      "[[ 117.06872736 -180.45558003  185.25274619  188.61118994]]\n",
      "[[ 117.17523079 -180.5441367   185.39742258  188.75555583]]\n",
      "[[ 117.28131153 -180.63221031  185.54147363  188.89926621]]\n",
      "[[ 117.38696983 -180.71981095  185.6849035   189.04232711]]\n",
      "The train loss is 0.002941781957480339\n",
      "The val loss is 0.003001656573867955\n",
      "[[ 117.48037902 -180.80784007  185.81826822  189.17584813]]\n",
      "[[ 117.5747062  -180.89359411  185.95168151  189.30896508]]\n",
      "[[ 117.66959382 -180.9776005   186.08497931  189.44163439]]\n",
      "[[ 117.76480578 -181.06021072  186.2180546   189.57382914]]\n",
      "[[ 117.86017931 -181.14166974  186.35083469  189.70553231]]\n",
      "[[ 117.95559858 -181.22215424  186.48326881  189.83673321]]\n",
      "[[ 118.05097915 -181.30179511  186.61532081  189.96742531]]\n",
      "[[ 118.14625828 -181.38069149  186.74696465  190.09760489]]\n",
      "[[ 118.24138875 -181.45891983  186.87818144  190.22727029]]\n",
      "[[ 118.33633457 -181.53654004  187.00895748  190.35642121]]\n",
      "The train loss is 0.0027303987780023086\n",
      "The val loss is 0.002834323252886507\n",
      "[[ 118.4310681  -181.61359975  187.13928295  190.48505845]]\n",
      "[[ 118.525568   -181.69013728  187.26915086  190.61318353]]\n",
      "[[ 118.61981771 -181.76618387  187.39855644  190.74079856]]\n",
      "[[ 118.71380436 -181.84176524  187.52749656  190.86790605]]\n",
      "[[ 118.80751795 -181.91690281  187.65596942  190.99450884]]\n",
      "[[ 118.90095075 -181.99161464  187.7839742   191.12061   ]]\n",
      "[[ 118.99409679 -182.06591607  187.91151087  191.24621277]]\n",
      "[[ 119.08695153 -182.1398203   188.03858003  191.37132051]]\n",
      "[[ 119.17951155 -182.21333878  188.16518277  191.49593668]]\n",
      "[[ 119.2625796  -182.28588213  188.28348765  191.61203627]]\n",
      "The train loss is 0.0025776958226442497\n",
      "The val loss is 0.0026852718441493016\n",
      "[[ 119.34600136 -182.35719777  188.40165883  191.72777978]]\n",
      "[[ 119.42965589 -182.4274688   188.51964272  191.84315502]]\n",
      "[[ 119.51345222 -182.49683434  188.63739962  191.95815369]]\n",
      "[[ 119.59732048 -182.56540248  188.75489956  192.07277014]]\n",
      "[[ 119.67512498 -182.62719473  188.86482037  192.18230579]]\n",
      "[[ 119.7531318  -182.6880927   188.97457477  192.29150542]]\n",
      "[[ 119.83126982 -182.74820574  189.08413205  192.40036303]]\n",
      "[[ 119.90948307 -182.80762079  189.19346842  192.50887449]]\n",
      "[[ 119.98772681 -182.86640806  189.30256522  192.61703701]]\n",
      "[[ 120.06596482 -182.92462504  189.41140766  192.72484883]]\n",
      "The train loss is 0.002412059421746312\n",
      "The val loss is 0.0025512505930781193\n",
      "[[ 120.14416748 -182.98231939  189.51998394  192.83230894]]\n",
      "[[ 120.22231033 -183.03953103  189.62828461  192.93941695]]\n",
      "[[ 120.30037299 -183.09629371  189.73630203  193.04617289]]\n",
      "[[ 120.3783384  -183.15263625  189.84403007  193.15257719]]\n",
      "[[ 120.45619218 -183.20858339  189.95146377  193.25863053]]\n",
      "[[ 120.53392211 -183.26415658  190.05859916  193.36433386]]\n",
      "[[ 120.61151784 -183.31937451  190.16543307  193.46968827]]\n",
      "[[ 120.68897047 -183.37425356  190.271963    193.57469503]]\n",
      "[[ 120.76627242 -183.4288082   190.37818701  193.67935551]]\n",
      "[[ 120.84341714 -183.48305126  190.48410361  193.7836712 ]]\n",
      "The train loss is 0.0022849999926730334\n",
      "The val loss is 0.0024304004160152935\n",
      "[[ 120.92039903 -183.53699418  190.58971173  193.88764365]]\n",
      "[[ 120.99721321 -183.59064721  190.69501061  193.99127447]]\n",
      "[[ 121.07385553 -183.64401959  190.79999979  194.09456534]]\n",
      "[[ 121.15032235 -183.69711967  190.90467906  194.19751797]]\n",
      "[[ 121.22661057 -183.74995504  191.00904841  194.30013412]]\n",
      "[[ 121.30271749 -183.8025326   191.11310801  194.40241554]]\n",
      "[[ 121.37864082 -183.85485872  191.21685819  194.50436405]]\n",
      "[[ 121.45437856 -183.9069392   191.32029943  194.60598145]]\n",
      "[[ 121.52992902 -183.95877944  191.4234323   194.70726957]]\n",
      "[[ 121.60529076 -184.0103844   191.5262575   194.80823023]]\n",
      "The train loss is 0.0021769547900206783\n",
      "The val loss is 0.0023213710827328176\n",
      "[[ 121.68046255 -184.06175873  191.6287758   194.90886528]]\n",
      "[[ 121.75544338 -184.11290674  191.73098807  195.00917656]]\n",
      "[[ 121.83023241 -184.16383247  191.83289522  195.10916591]]\n",
      "[[ 121.90482894 -184.21453971  191.93449824  195.20883518]]\n",
      "[[ 121.97923243 -184.26503201  192.03579817  195.3081862 ]]\n",
      "[[ 122.05344245 -184.31531275  192.13679611  195.40722082]]\n",
      "[[ 122.12745868 -184.36538512  192.23749317  195.50594087]]\n",
      "[[ 122.20128091 -184.41525211  192.33789051  195.60434817]]\n",
      "[[ 122.27217545 -184.46090256  192.43203693  195.69575054]]\n",
      "[[ 122.34296226 -184.50626234  192.52594631  195.786882  ]]\n",
      "The train loss is 0.002080250365194738\n",
      "The val loss is 0.002222451802725648\n",
      "[[ 122.41363444 -184.55134443  192.6196167   195.87774343]]\n",
      "[[ 122.48418595 -184.59616047  192.71304649  195.96833576]]\n",
      "[[ 122.55461148 -184.64072093  192.8062344   196.05865998]]\n",
      "[[ 122.62490637 -184.68503532  192.8991794   196.14871715]]\n",
      "[[ 122.69506651 -184.72911225  192.99188072  196.23850836]]\n",
      "[[ 122.76508826 -184.77295957  193.08433778  196.32803473]]\n",
      "[[ 122.83496843 -184.81658447  193.17655019  196.41729741]]\n",
      "[[ 122.90470421 -184.85999354  193.26851769  196.50629758]]\n",
      "[[ 122.97429309 -184.90319284  193.36024019  196.59503643]]\n",
      "[[ 123.04373289 -184.94618795  193.4517177   196.68351518]]\n",
      "The train loss is 0.00198923850616927\n",
      "The val loss is 0.0021320870950021233\n",
      "[[ 123.11302168 -184.98898406  193.54295034  196.77173504]]\n",
      "[[ 123.18215775 -185.03158597  193.63393834  196.85969727]]\n",
      "[[ 123.25113962 -185.07399815  193.724682    196.94740309]]\n",
      "[[ 123.31996598 -185.11622475  193.81518169  197.03485377]]\n",
      "[[ 123.38863571 -185.15826967  193.90543786  197.12205055]]\n",
      "[[ 123.4571478  -185.20013656  193.99545102  197.20899469]]\n",
      "[[ 123.52550143 -185.24182884  194.08522171  197.29568747]]\n",
      "[[ 123.59369587 -185.28334973  194.17475054  197.38213014]]\n",
      "[[ 123.6617305  -185.32470227  194.26403816  197.46832395]]\n",
      "[[ 123.72960482 -185.36588933  194.35308526  197.55427019]]\n",
      "The train loss is 0.0019082605090867282\n",
      "The val loss is 0.0020491658111054904\n",
      "[[ 123.7973184  -185.40691363  194.44189255  197.6399701 ]]\n",
      "[[ 123.86487092 -185.44777774  194.53046079  197.72542495]]\n",
      "[[ 123.93226212 -185.48848411  194.61879074  197.81063599]]\n",
      "[[ 123.9994918  -185.52903507  194.70688321  197.89560448]]\n",
      "[[ 124.06655985 -185.56943284  194.79473903  197.98033166]]\n",
      "[[ 124.1334662  -185.60967954  194.88235903  198.06481879]]\n",
      "[[ 124.20021083 -185.64977719  194.96974409  198.14906711]]\n",
      "[[ 124.26679378 -185.68972774  195.05689507  198.23307784]]\n",
      "[[ 124.33321513 -185.72953305  195.14381287  198.31685224]]\n",
      "[[ 124.399475   -185.76919491  195.23049839  198.40039152]]\n",
      "The train loss is 0.001834760671279328\n",
      "The val loss is 0.0019727814251592\n",
      "[[ 124.46557355 -185.80871503  195.31695255  198.48369691]]\n",
      "[[ 124.53151098 -185.84809508  195.40317627  198.56676962]]\n",
      "[[ 124.59728751 -185.88733665  195.48917049  198.64961088]]\n",
      "[[ 124.6629034  -185.92644128  195.57493615  198.73222187]]\n",
      "[[ 124.72835892 -185.96541047  195.66047419  198.81460382]]\n",
      "[[ 124.79365439 -186.00424564  195.74578557  198.8967579 ]]\n",
      "[[ 124.85879014 -186.04294821  195.83087125  198.97868531]]\n",
      "[[ 124.92376652 -186.08151951  195.91573219  199.06038723]]\n",
      "[[ 124.9885839  -186.11996087  196.00036937  199.14186484]]\n",
      "[[ 125.05324267 -186.15827356  196.08478374  199.2231193 ]]\n",
      "The train loss is 0.0017673565043064222\n",
      "The val loss is 0.0019021498359072936\n",
      "[[ 125.11774323 -186.19645881  196.16897629  199.30415178]]\n",
      "[[ 125.18208602 -186.23451783  196.25294798  199.38496344]]\n",
      "[[ 125.24627145 -186.2724518   196.3366998   199.46555543]]\n",
      "[[ 125.31029999 -186.31026187  196.42023272  199.54592889]]\n",
      "[[ 125.37417209 -186.34794913  196.50354771  199.62608495]]\n",
      "[[ 125.43788822 -186.3855147   196.58664576  199.70602476]]\n",
      "[[ 125.50144887 -186.42295962  196.66952784  199.78574943]]\n",
      "[[ 125.56485453 -186.46028493  196.75219492  199.86526009]]\n",
      "[[ 125.6281057  -186.49749166  196.83464799  199.94455783]]\n",
      "[[ 125.69120288 -186.53458079  196.91688802  200.02364378]]\n",
      "The train loss is 0.0017051337256664338\n",
      "The val loss is 0.0018366072548551906\n",
      "[[ 125.7541466  -186.5715533   196.99891597  200.10251902]]\n",
      "[[ 125.81693737 -186.60841014  197.08073283  200.18118464]]\n",
      "[[ 125.87957572 -186.64515224  197.16233955  200.25964174]]\n",
      "[[ 125.94206219 -186.68178052  197.24373712  200.33789138]]\n",
      "[[ 126.00439731 -186.71829587  197.32492648  200.41593464]]\n",
      "[[ 126.06658163 -186.75469919  197.40590861  200.49377259]]\n",
      "[[ 126.1286157  -186.79099132  197.48668445  200.57140628]]\n",
      "[[ 126.19050006 -186.82717312  197.56725498  200.64883676]]\n",
      "[[ 126.25223527 -186.86324543  197.64762114  200.72606508]]\n",
      "[[ 126.31382189 -186.89920906  197.72778388  200.80309227]]\n",
      "The train loss is 0.00164741687272229\n",
      "The val loss is 0.0017755922110645498\n",
      "[[ 126.37526048 -186.93506482  197.80774416  200.87991938]]\n",
      "[[ 126.4365516  -186.97081351  197.8875029   200.95654741]]\n",
      "[[ 126.49769582 -187.0064559   197.96706106  201.03297739]]\n",
      "[[ 126.5586937  -187.04199275  198.04641957  201.10921034]]\n",
      "[[ 126.61954582 -187.07742484  198.12557936  201.18524725]]\n",
      "[[ 126.68025273 -187.1127529   198.20454136  201.26108913]]\n",
      "[[ 126.74081501 -187.14797766  198.28330651  201.33673696]]\n",
      "[[ 126.80123324 -187.18309985  198.3618757   201.41219174]]\n",
      "[[ 126.86150798 -187.21812018  198.44024988  201.48745445]]\n",
      "[[ 126.9216398  -187.25303935  198.51842994  201.56252605]]\n",
      "The train loss is 0.0015936744413092582\n",
      "The val loss is 0.0017186272683333659\n",
      "[[ 126.98162928 -187.28785806  198.5964168   201.63740751]]\n",
      "[[ 127.041477   -187.32257698  198.67421136  201.7120998 ]]\n",
      "[[ 127.10118352 -187.35719679  198.75181453  201.78660387]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "HelperFunctions.ipynb:3: RuntimeWarning: overflow encountered in exp\n",
      "  {\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 127.16074941 -187.39171815  198.82922719  201.86092066]]\n",
      "[[ 127.22017525 -187.42614172  198.90645025  201.93505113]]\n",
      "[[ 127.27946162 -187.46046815  198.98348459  202.00899619]]\n",
      "[[ 127.33860907 -187.49469807  199.06033109  202.08275679]]\n",
      "[[ 127.39761818 -187.52883211  199.13699063  202.15633385]]\n",
      "[[ 127.45648953 -187.56287091  199.21346408  202.22972828]]\n",
      "[[ 127.51522367 -187.59681507  199.28975233  202.30294099]]\n",
      "The train loss is 0.0015434720456037924\n",
      "The val loss is 0.001665303602429924\n",
      "[[ 127.57382118 -187.6306652   199.36585623  202.3759729 ]]\n",
      "[[ 127.63228262 -187.6644219   199.44177664  202.44882489]]\n",
      "[[ 127.69060856 -187.69808577  199.51751441  202.52149787]]\n",
      "[[ 127.74879956 -187.73165738  199.59307042  202.59399271]]\n",
      "[[ 127.80685618 -187.76513733  199.66844549  202.66631031]]\n",
      "[[ 127.86477899 -187.79852618  199.74364047  202.73845153]]\n",
      "[[ 127.92256854 -187.8318245   199.81865621  202.81041725]]\n",
      "[[ 127.98022539 -187.86503285  199.89349353  202.88220833]]\n",
      "[[ 128.0377501  -187.89815179  199.96815326  202.95382563]]\n",
      "[[ 128.09514323 -187.93118185  200.04263624  203.02527   ]]\n",
      "The train loss is 0.0014964459200980317\n",
      "The val loss is 0.0016152686236310823\n",
      "[[ 128.15240531 -187.96412359  200.11694327  203.09654228]]\n",
      "[[ 128.20953691 -187.99697753  200.19107518  203.16764333]]\n",
      "[[ 128.26653858 -188.02974422  200.26503278  203.23857396]]\n",
      "[[ 128.32341086 -188.06242417  200.33881687  203.30933503]]\n",
      "[[ 128.3801543  -188.0950179   200.41242826  203.37992734]]\n",
      "[[ 128.43676943 -188.12752592  200.48586773  203.45035172]]\n",
      "[[ 128.49325681 -188.15994875  200.5591361   203.52060898]]\n",
      "[[ 128.54961696 -188.19228688  200.63223414  203.59069993]]\n",
      "[[ 128.60585043 -188.22454082  200.70516264  203.66062538]]\n",
      "[[ 128.66195776 -188.25671106  200.77792237  203.73038611]]\n",
      "The train loss is 0.0014522862963163466\n",
      "The val loss is 0.0015682161667189226\n",
      "[[ 128.71793947 -188.28879808  200.85051412  203.79998293]]\n",
      "[[ 128.77379609 -188.32080237  200.92293865  203.86941661]]\n",
      "[[ 128.82952816 -188.35272441  200.99519673  203.93868794]]\n",
      "[[ 128.8851362  -188.38456467  201.06728912  204.00779771]]\n",
      "[[ 128.94062073 -188.41632361  201.13921658  204.07674666]]\n",
      "[[ 128.99598228 -188.44800171  201.21097986  204.14553559]]\n",
      "[[ 129.05122136 -188.47959943  201.28257971  204.21416524]]\n",
      "[[ 129.10633849 -188.51111721  201.35401687  204.28263637]]\n",
      "[[ 129.16133419 -188.54255551  201.42529209  204.35094973]]\n",
      "[[ 129.21620897 -188.57391478  201.49640609  204.41910607]]\n",
      "The train loss is 0.0014107261389979203\n",
      "The val loss is 0.0015238787080044255\n",
      "[[ 129.27096333 -188.60519546  201.56735961  204.48710613]]\n",
      "[[ 129.3255978  -188.63639798  201.63815338  204.55495065]]\n",
      "[[ 129.38011287 -188.6675228   201.70878812  204.62264035]]\n",
      "[[ 129.43450904 -188.69857032  201.77926453  204.69017597]]\n",
      "[[ 129.48878682 -188.72954098  201.84958335  204.75755822]]\n",
      "[[ 129.5429467  -188.76043521  201.91974528  204.82478782]]\n",
      "[[ 129.59698918 -188.79125342  201.98975102  204.89186549]]\n",
      "[[ 129.65091476 -188.82199603  202.05960127  204.95879193]]\n",
      "[[ 129.70472392 -188.85266345  202.12929674  205.02556784]]\n",
      "[[ 129.75841716 -188.88325608  202.19883811  205.09219393]]\n",
      "The train loss is 0.0013715330686397424\n",
      "The val loss is 0.0014820211511756987\n",
      "[[ 129.81199496 -188.91377433  202.26822607  205.15867088]]\n",
      "[[ 129.8654578  -188.9442186   202.33746131  205.22499938]]\n",
      "[[ 129.91880617 -188.9745893   202.4065445   205.29118013]]\n",
      "[[ 129.97204055 -189.0048868   202.47547633  205.35721379]]\n",
      "[[ 130.0251614  -189.03511151  202.54425746  205.42310105]]\n",
      "[[ 130.07816922 -189.0652638   202.61288857  205.48884258]]\n",
      "[[ 130.12505182 -189.09658925  202.67676256  205.54991649]]\n",
      "[[ 130.17188072 -189.12777716  202.74051987  205.61086309]]\n",
      "[[ 130.21865412 -189.15883142  202.8041602   205.67168285]]\n",
      "[[ 130.26537041 -189.18975566  202.86768331  205.73237621]]\n",
      "The train loss is 0.0013334401206770569\n",
      "The val loss is 0.0014424327645576425\n",
      "[[ 130.31202808 -189.22055329  202.93108901  205.79294365]]\n",
      "[[ 130.35862574 -189.25122754  202.99437715  205.85338562]]\n",
      "[[ 130.40516212 -189.28178142  203.05754763  205.91370261]]\n",
      "[[ 130.45163606 -189.3122178   203.12060038  205.97389508]]\n",
      "[[ 130.49804648 -189.34253937  203.18353538  206.03396351]]\n",
      "[[ 130.54439238 -189.3727487   203.24635262  206.09390837]]\n",
      "[[ 130.59067285 -189.40284821  203.30905214  206.15373015]]\n",
      "[[ 130.63688704 -189.43284019  203.371634    206.21342931]]\n",
      "[[ 130.68303419 -189.46272682  203.43409826  206.27300634]]\n",
      "[[ 130.72911357 -189.49251019  203.49644505  206.3324617 ]]\n",
      "The train loss is 0.0012958456821373758\n",
      "The val loss is 0.0014049053139902985\n",
      "[[ 130.77512454 -189.52219226  203.55867448  206.39179588]]\n",
      "[[ 130.82106648 -189.55177492  203.62078668  206.45100936]]\n",
      "[[ 130.86693884 -189.58125997  203.68278183  206.5101026 ]]\n",
      "[[ 130.91274111 -189.61064912  203.74466009  206.56907608]]\n",
      "[[ 130.95847283 -189.63994401  203.80642166  206.62793027]]\n",
      "[[ 131.00413356 -189.66914621  203.86806672  206.68666563]]\n",
      "[[ 131.0459718  -189.69793489  203.92538373  206.74096567]]\n",
      "[[ 131.08776389 -189.72661026  203.9826024   206.79516011]]\n",
      "[[ 131.12950834 -189.75517545  204.03972245  206.84924928]]\n",
      "[[ 131.17120378 -189.7836334   204.09674364  206.90323349]]\n",
      "The train loss is 0.0012606562861805066\n",
      "The val loss is 0.001369291391276197\n",
      "[[ 131.21284893 -189.81198689  204.15366575  206.9571131 ]]\n",
      "[[ 131.25444263 -189.84023852  204.21048863  207.01088844]]\n",
      "[[ 131.29598379 -189.86839076  204.26721213  207.06455984]]\n",
      "[[ 131.33747141 -189.89644594  204.32383616  207.11812764]]\n",
      "[[ 131.37890456 -189.92440625  204.38036065  207.1715922 ]]\n",
      "[[ 131.42028238 -189.95227378  204.43678554  207.22495385]]\n",
      "[[ 131.46160409 -189.9800505   204.49311082  207.27821294]]\n",
      "[[ 131.50286894 -190.00773829  204.54933647  207.33136983]]\n",
      "[[ 131.54407625 -190.03533893  204.60546253  207.38442484]]\n",
      "[[ 131.5852254  -190.06285411  204.66148903  207.43737834]]\n",
      "The train loss is 0.0012272779393169328\n",
      "The val loss is 0.0013354517453987222\n",
      "[[ 131.62631581 -190.09028545  204.71741603  207.49023068]]\n",
      "[[ 131.66734694 -190.11763448  204.77324359  207.54298219]]\n",
      "[[ 131.70831829 -190.14490266  204.8289718   207.59563322]]\n",
      "[[ 131.7492294  -190.17209141  204.88460077  207.64818414]]\n",
      "[[ 131.79007985 -190.19920205  204.9401306   207.70063527]]\n",
      "[[ 131.83086925 -190.22623586  204.99556142  207.75298697]]\n",
      "[[ 131.87159725 -190.25319407  205.05089336  207.80523958]]\n",
      "[[ 131.91226351 -190.28007786  205.10612657  207.85739345]]\n",
      "[[ 131.95286774 -190.30688834  205.1612612   207.90944893]]\n",
      "[[ 131.99340965 -190.33362659  205.21629741  207.96140634]]\n",
      "The train loss is 0.0011962618203718916\n",
      "The val loss is 0.0013032670645151123\n",
      "[[ 132.03388899 -190.36029366  205.27123538  208.01326605]]\n",
      "[[ 132.07430554 -190.38689053  205.32607528  208.06502838]]\n",
      "[[ 132.11465908 -190.41341816  205.3808173   208.11669368]]\n",
      "[[ 132.15494943 -190.43987749  205.43546162  208.16826229]]\n",
      "[[ 132.19517642 -190.46626938  205.49000845  208.21973455]]\n",
      "[[ 132.23533989 -190.4925947   205.54445798  208.27111079]]\n",
      "[[ 132.27543971 -190.51885427  205.59881043  208.32239135]]\n",
      "[[ 132.31547576 -190.54504889  205.65306601  208.37357656]]\n",
      "[[ 132.35544793 -190.57117933  205.70722494  208.42466675]]\n",
      "[[ 132.39535614 -190.59724632  205.76128743  208.47566227]]\n",
      "The train loss is 0.001167159524979415\n",
      "The val loss is 0.0012726243988833768\n",
      "[[ 132.4352003  -190.62325058  205.81525371  208.52656343]]\n",
      "[[ 132.47498035 -190.64919281  205.869124    208.57737058]]\n",
      "[[ 132.51469623 -190.67507368  205.92289855  208.62808403]]\n",
      "[[ 132.55434791 -190.70089384  205.97657759  208.67870411]]\n",
      "[[ 132.59393535 -190.72665391  206.03016135  208.72923116]]\n",
      "[[ 132.63345852 -190.75235451  206.08365007  208.77966548]]\n",
      "[[ 132.67291743 -190.77799623  206.137044    208.83000742]]\n",
      "[[ 132.71231205 -190.80357964  206.19034338  208.88025728]]\n",
      "[[ 132.7516424  -190.82910531  206.24354846  208.9304154 ]]\n",
      "[[ 132.79090849 -190.85457376  206.29665949  208.98048208]]\n",
      "The train loss is 0.0011396874984262195\n",
      "The val loss is 0.0012434169687292216\n",
      "[[ 132.83011034 -190.87998554  206.34967672  209.03045765]]\n",
      "[[ 132.86924797 -190.90534114  206.4026004   209.08034242]]\n",
      "[[ 132.90832143 -190.93064107  206.45543078  209.13013671]]\n",
      "[[ 132.94733075 -190.95588581  206.50816813  209.17984083]]\n",
      "[[ 132.98627599 -190.98107584  206.56081269  209.2294551 ]]\n",
      "[[ 133.02515718 -191.00621161  206.61336473  209.27897982]]\n",
      "[[ 133.0639744  -191.03129356  206.6658245   209.32841531]]\n",
      "[[ 133.1027277  -191.05632215  206.71819226  209.37776187]]\n",
      "[[ 133.14141716 -191.08129778  206.77046828  209.42701981]]\n",
      "[[ 133.18004284 -191.10622088  206.82265282  209.47618945]]\n",
      "The train loss is 0.0011136470647574762\n",
      "The val loss is 0.0012155457937201642\n",
      "[[ 133.21860484 -191.13109185  206.87474613  209.52527107]]\n",
      "[[ 133.25710322 -191.15591109  206.92674848  209.57426499]]\n",
      "[[ 133.29553808 -191.18067898  206.97866014  209.62317151]]\n",
      "[[ 133.33390951 -191.2053959   207.03048136  209.67199092]]\n",
      "[[ 133.3722176  -191.23006222  207.08221241  209.72072354]]\n",
      "[[ 133.41046244 -191.2546783   207.13385356  209.76936965]]\n",
      "[[ 133.44864415 -191.27924449  207.18540507  209.81792955]]\n",
      "[[ 133.48676283 -191.30376114  207.2368672   209.86640355]]\n",
      "[[ 133.52481857 -191.32822859  207.28824022  209.91479192]]\n",
      "[[ 133.5628115  -191.35264716  207.3395244   209.96309498]]\n",
      "The train loss is 0.0010888884780500738\n",
      "The val loss is 0.0011889199303044588\n",
      "[[ 133.60074173 -191.37701719  207.39072     210.011313  ]]\n",
      "[[ 133.63860937 -191.40133899  207.44182727  210.05944628]]\n",
      "[[ 133.67641454 -191.42561287  207.4928465   210.10749511]]\n",
      "[[ 133.71415736 -191.44983913  207.54377794  210.15545978]]\n",
      "[[ 133.75183795 -191.47401808  207.59462186  210.20334057]]\n",
      "[[ 133.78945644 -191.49815002  207.64537853  210.25113776]]\n",
      "[[ 133.82701295 -191.52223522  207.6960482   210.29885166]]\n",
      "[[ 133.86450762 -191.54627397  207.74663114  210.34648252]]\n",
      "[[ 133.90194058 -191.57026655  207.79712762  210.39403065]]\n",
      "[[ 133.93931195 -191.59421324  207.8475379   210.44149632]]\n",
      "The train loss is 0.0010652928278345138\n",
      "The val loss is 0.0011634561337696217\n",
      "[[ 133.97662187 -191.61811429  207.89786224  210.4888798 ]]\n",
      "[[ 134.01387048 -191.64196999  207.9481009   210.53618138]]\n",
      "[[ 134.05105791 -191.66578057  207.99825416  210.58340134]]\n",
      "[[ 134.0881843  -191.68954631  208.04832226  210.63053994]]\n",
      "[[ 134.1252498  -191.71326744  208.09830547  210.67759747]]\n",
      "[[ 134.16225453 -191.73694423  208.14820406  210.7245742 ]]\n",
      "[[ 134.19919865 -191.7605769   208.19801828  210.7714704 ]]\n",
      "[[ 134.2360823  -191.7841657   208.24774839  210.81828634]]\n",
      "[[ 134.27290561 -191.80771086  208.29739466  210.86502229]]\n",
      "[[ 134.30966875 -191.83121262  208.34695734  210.91167852]]\n",
      "The train loss is 0.0010427620758103676\n",
      "The val loss is 0.001139078296382006\n",
      "[[ 134.34637184 -191.8546712   208.39643669  210.9582553 ]]\n",
      "[[ 134.38301504 -191.87808683  208.44583297  211.0047529 ]]\n",
      "[[ 134.4195985  -191.90145973  208.49514644  211.05117157]]\n",
      "[[ 134.45612236 -191.92479011  208.54437735  211.09751159]]\n",
      "[[ 134.49258678 -191.9480782   208.59352596  211.14377321]]\n",
      "[[ 134.5289919  -191.9713242   208.64259253  211.1899567 ]]\n",
      "[[ 134.56533788 -191.99452833  208.6915773   211.23606232]]\n",
      "[[ 134.60162485 -192.01769079  208.74048055  211.28209033]]\n",
      "[[ 134.63785299 -192.04081177  208.78930251  211.32804099]]\n",
      "[[ 134.67402243 -192.0638915   208.83804344  211.37391455]]\n",
      "The train loss is 0.0010212131497518981\n",
      "The val loss is 0.001115716820309536\n",
      "[[ 134.71013333 -192.08693015  208.8867036   211.41971127]]\n",
      "[[ 134.74618585 -192.10992794  208.93528323  211.4654314 ]]\n",
      "[[ 134.78218013 -192.13288504  208.98378259  211.51107521]]\n",
      "[[ 134.81811633 -192.15580166  209.03220193  211.55664294]]\n",
      "[[ 134.8539946  -192.17867798  209.0805415   211.60213484]]\n",
      "[[ 134.8898151  -192.20151418  209.12880154  211.64755117]]\n",
      "[[ 134.92557799 -192.22431046  209.1769823   211.69289217]]\n",
      "[[ 134.96128341 -192.24706699  209.22508404  211.73815809]]\n",
      "[[ 134.99693151 -192.26978394  209.27310699  211.78334919]]\n",
      "[[ 135.03252247 -192.29246151  209.3210514   211.8284657 ]]\n",
      "The train loss is 0.0010005742185473635\n",
      "The val loss is 0.001093307997704693\n",
      "[[ 135.06805643 -192.31509986  209.36891753  211.87350787]]\n",
      "[[ 135.10353354 -192.33769917  209.41670561  211.91847595]]\n",
      "[[ 135.13895396 -192.3602596   209.46441588  211.96337018]]\n",
      "[[ 135.17431785 -192.38278134  209.51204859  212.00819079]]\n",
      "[[ 135.20962536 -192.40526454  209.55960399  212.05293804]]\n",
      "[[ 135.24487664 -192.42770937  209.6070823   212.09761217]]\n",
      "[[ 135.28007186 -192.450116    209.65448378  212.1422134 ]]\n",
      "[[ 135.31521117 -192.47248459  209.70180866  212.18674198]]\n",
      "[[ 135.35029471 -192.4948153   209.74905718  212.23119815]]\n",
      "[[ 135.38532266 -192.51710829  209.79622958  212.27558214]]\n",
      "The train loss is 0.000980782215571096\n",
      "The val loss is 0.001071793429636999\n",
      "[[ 135.42029516 -192.53936371  209.8433261   212.31989419]]\n",
      "[[ 135.45521236 -192.56158172  209.89034697  212.36413452]]\n",
      "[[ 135.49007443 -192.58376248  209.93729243  212.40830338]]\n",
      "[[ 135.52488152 -192.60590614  209.98416271  212.45240099]]\n",
      "[[ 135.55963378 -192.62801285  210.03095804  212.49642759]]\n",
      "[[ 135.59433136 -192.65008276  210.07767867  212.5403834 ]]\n",
      "[[ 135.62897443 -192.67211601  210.12432482  212.58426866]]\n",
      "[[ 135.66356313 -192.69411277  210.17089672  212.62808358]]\n",
      "[[ 135.69809762 -192.71607316  210.21739461  212.67182841]]\n",
      "[[ 135.73257806 -192.73799734  210.26381871  212.71550336]]\n",
      "The train loss is 0.000961781115879422\n",
      "The val loss is 0.0010511194958712456\n",
      "[[ 135.76700459 -192.75988545  210.31016925  212.75910866]]\n",
      "[[ 135.80137737 -192.78173763  210.35644647  212.80264453]]\n",
      "[[ 135.83569655 -192.80355403  210.40265059  212.84611119]]\n",
      "[[ 135.86996229 -192.82533477  210.44878183  212.88950887]]\n",
      "[[ 135.90417474 -192.84708     210.49484042  212.93283779]]\n",
      "[[ 135.93833405 -192.86878986  210.54082659  212.97609817]]\n",
      "[[ 135.97244037 -192.89046448  210.58674055  213.01929022]]\n",
      "[[ 136.00649385 -192.91210399  210.63258254  213.06241417]]\n",
      "[[ 136.04049466 -192.93370854  210.67835278  213.10547023]]\n",
      "[[ 136.07444293 -192.95527824  210.72405148  213.14845861]]\n",
      "The train loss is 0.0009435206903372721\n",
      "The val loss is 0.0010312368779185777\n",
      "[[ 136.10833881 -192.97681324  210.76967888  213.19137954]]\n",
      "[[ 136.14218247 -192.99831367  210.81523518  213.23423323]]\n",
      "[[ 136.17597404 -193.01977964  210.8607206   213.27701989]]\n",
      "[[ 136.20971369 -193.0412113   210.90613538  213.31973973]]\n",
      "[[ 136.24340155 -193.06260877  210.95147971  213.36239296]]\n",
      "[[ 136.27703778 -193.08397217  210.99675383  213.4049798 ]]\n",
      "[[ 136.31062253 -193.10530162  211.04195794  213.44750045]]\n",
      "[[ 136.34415594 -193.12659727  211.08709226  213.48995513]]\n",
      "[[ 136.37763816 -193.14785921  211.132157    213.53234404]]\n",
      "[[ 136.41106935 -193.16908759  211.17715239  213.57466738]]\n",
      "The train loss is 0.0009259555741062433\n",
      "The val loss is 0.0010121001332710521\n",
      "[[ 136.44444964 -193.19028252  211.22207862  213.61692537]]\n",
      "[[ 136.47777919 -193.21144412  211.26693592  213.65911821]]\n",
      "[[ 136.51105815 -193.23257251  211.31172448  213.7012461 ]]\n",
      "[[ 136.54428665 -193.25366781  211.35644454  213.74330925]]\n",
      "[[ 136.57746484 -193.27473014  211.40109628  213.78530786]]\n",
      "[[ 136.61059287 -193.29575961  211.44567993  213.82724212]]\n",
      "[[ 136.64367089 -193.31675635  211.49019568  213.86911225]]\n",
      "[[ 136.67669904 -193.33772046  211.53464375  213.91091844]]\n",
      "[[ 136.70967746 -193.35865206  211.57902434  213.95266089]]\n",
      "[[ 136.7426063  -193.37955126  211.62333766  213.9943398 ]]\n",
      "The train loss is 0.0009090445499860753\n",
      "The val loss is 0.000993667316769712\n",
      "[[ 136.7754857  -193.40041818  211.66758391  214.03595537]]\n",
      "[[ 136.80831581 -193.42125293  211.71176329  214.07750779]]\n",
      "[[ 136.84109676 -193.44205563  211.75587601  214.11899725]]\n",
      "[[ 136.8738287  -193.46282637  211.79992227  214.16042396]]\n",
      "[[ 136.90651177 -193.48356528  211.84390227  214.2017881 ]]\n",
      "[[ 136.93914612 -193.50427246  211.88781622  214.24308987]]\n",
      "[[ 136.97173187 -193.52494802  211.9316643   214.28432947]]\n",
      "[[ 137.00426918 -193.54559207  211.97544672  214.32550708]]\n",
      "[[ 137.03675819 -193.56620471  212.01916369  214.36662289]]\n",
      "[[ 137.06919903 -193.58678605  212.06281538  214.40767709]]\n",
      "The train loss is 0.000892749983404387\n",
      "The val loss is 0.0009758996443920623\n",
      "[[ 137.10159184 -193.6073362   212.10640201  214.44866988]]\n",
      "[[ 137.13393676 -193.62785526  212.14992377  214.48960144]]\n",
      "[[ 137.16623393 -193.64834334  212.19338085  214.53047195]]\n",
      "[[ 137.19848349 -193.66880054  212.23677345  214.57128161]]\n",
      "[[ 137.23068558 -193.68922697  212.28010175  214.6120306 ]]\n",
      "[[ 137.26284032 -193.70962271  212.32336596  214.65271911]]\n",
      "[[ 137.29494787 -193.72998789  212.36656627  214.69334731]]\n",
      "[[ 137.32700835 -193.7503226   212.40970286  214.7339154 ]]\n",
      "[[ 137.3590219  -193.77062693  212.45277592  214.77442355]]\n",
      "[[ 137.39098866 -193.790901    212.49578566  214.81487195]]\n",
      "The train loss is 0.0008770373675595522\n",
      "The val loss is 0.0009587611947298066\n",
      "[[ 137.42290875 -193.81114489  212.53873224  214.85526078]]\n",
      "[[ 137.45478232 -193.83135871  212.58161587  214.89559022]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 137.48660951 -193.85154256  212.62443674  214.93586044]]\n",
      "[[ 137.51839043 -193.87169653  212.66719501  214.97607163]]\n",
      "[[ 137.55012523 -193.89182072  212.70989089  215.01622396]]\n",
      "[[ 137.58181403 -193.91191523  212.75252456  215.05631762]]\n",
      "[[ 137.61345698 -193.93198015  212.7950962   215.09635277]]\n",
      "[[ 137.6450542  -193.95201558  212.837606    215.13632959]]\n",
      "[[ 137.67660582 -193.97202161  212.88005413  215.17624827]]\n",
      "[[ 137.70811197 -193.99199834  212.92244079  215.21610897]]\n",
      "The train loss is 0.0008618749506436168\n",
      "The val loss is 0.0009422186437150302\n",
      "[[ 137.73957279 -194.01194586  212.96476614  215.25591186]]\n",
      "[[ 137.7709884  -194.03186427  213.00703038  215.29565712]]\n",
      "[[ 137.80235894 -194.05175365  213.04923369  215.33534493]]\n",
      "[[ 137.83368452 -194.0716141   213.09137623  215.37497544]]\n",
      "[[ 137.86496529 -194.09144572  213.1334582   215.41454884]]\n",
      "[[ 137.89620137 -194.11124858  213.17547976  215.45406529]]\n",
      "[[ 137.92739288 -194.13102279  213.2174411   215.49352496]]\n",
      "[[ 137.95853995 -194.15076843  213.2593424   215.53292802]]\n",
      "[[ 137.98964272 -194.17048559  213.30118382  215.57227464]]\n",
      "[[ 138.0207013  -194.19017437  213.34296554  215.61156498]]\n",
      "The train loss is 0.0008472334256368163\n",
      "The val loss is 0.0009262410285675068\n",
      "[[ 138.05171582 -194.20983484  213.38468774  215.65079922]]\n",
      "[[ 138.08268641 -194.2294671   213.42635059  215.68997751]]\n",
      "[[ 138.1136132  -194.24907124  213.46795426  215.72910002]]\n",
      "[[ 138.1444963  -194.26864735  213.50949893  215.76816691]]\n",
      "[[ 138.17533584 -194.2881955   213.55098476  215.80717836]]\n",
      "[[ 138.20613194 -194.30771579  213.59241194  215.84613452]]\n",
      "[[ 138.23688474 -194.3272083   213.63378062  215.88503554]]\n",
      "[[ 138.26759434 -194.34667312  213.67509097  215.92388161]]\n",
      "[[ 138.29826087 -194.36611033  213.71634318  215.96267287]]\n",
      "[[ 138.32888446 -194.38552002  213.7575374   216.00140948]]\n",
      "The train loss is 0.0008330856687772426\n",
      "The val loss is 0.0009107995373841436\n",
      "[[ 138.35946523 -194.40490227  213.79867379  216.04009161]]\n",
      "[[ 138.39000329 -194.42425716  213.83975254  216.07871942]]\n",
      "[[ 138.42049877 -194.44358479  213.8807738   216.11729305]]\n",
      "[[ 138.45095178 -194.46288522  213.92173774  216.15581268]]\n",
      "[[ 138.48136245 -194.48215855  213.96264453  216.19427845]]\n",
      "[[ 138.5117309  -194.50140485  214.00349432  216.23269052]]\n",
      "[[ 138.54205724 -194.52062421  214.04428729  216.27104905]]\n",
      "[[ 138.5723416  -194.53981671  214.08502358  216.30935419]]\n",
      "[[ 138.60258408 -194.55898242  214.12570338  216.3476061 ]]\n",
      "[[ 138.63278481 -194.57812144  214.16632683  216.38580493]]\n",
      "The train loss is 0.0008194065165770919\n",
      "The val loss is 0.000895867321221586\n",
      "[[ 138.66294391 -194.59723383  214.2068941   216.42395083]]\n",
      "[[ 138.69306149 -194.61631969  214.24740535  216.46204396]]\n",
      "[[ 138.72313766 -194.63537908  214.28786074  216.50008446]]\n",
      "[[ 138.75317255 -194.65441208  214.32826043  216.53807249]]\n",
      "[[ 138.78316627 -194.67341879  214.36860457  216.57600819]]\n",
      "[[ 138.81311893 -194.69239926  214.40889332  216.61389173]]\n",
      "[[ 138.84303065 -194.71135358  214.44912685  216.65172324]]\n",
      "[[ 138.87290154 -194.73028183  214.48930529  216.68950287]]\n",
      "[[ 138.90273172 -194.74918409  214.52942883  216.72723078]]\n",
      "[[ 138.93252129 -194.76806043  214.56949759  216.76490711]]\n",
      "The train loss is 0.0008061725738640885\n",
      "The val loss is 0.0008814193259289658\n",
      "[[ 138.96227038 -194.78691092  214.60951175  216.80253201]]\n",
      "[[ 138.99197909 -194.80573565  214.64947146  216.84010561]]\n",
      "[[ 139.02164753 -194.82453468  214.68937686  216.87762808]]\n",
      "[[ 139.05127582 -194.8433081   214.72922811  216.91509955]]\n",
      "[[ 139.08086407 -194.86205597  214.76902537  216.95252016]]\n",
      "[[ 139.11041238 -194.88077838  214.80876878  216.98989007]]\n",
      "[[ 139.13992088 -194.89947539  214.8484585   217.02720941]]\n",
      "[[ 139.16938966 -194.91814708  214.88809467  217.06447833]]\n",
      "[[ 139.19881884 -194.93679352  214.92767744  217.10169696]]\n",
      "[[ 139.22820853 -194.95541479  214.96720697  217.13886546]]\n",
      "The train loss is 0.0007933620471385799\n",
      "The val loss is 0.0008674321413381425\n",
      "[[ 139.25755883 -194.97401096  215.0066834   217.17598395]]\n",
      "[[ 139.28686986 -194.99258209  215.04610688  217.21305258]]\n",
      "[[ 139.31614172 -195.01112827  215.08547756  217.2500715 ]]\n",
      "[[ 139.34537451 -195.02964956  215.12479559  217.28704083]]\n",
      "[[ 139.37456836 -195.04814604  215.1640611   217.32396072]]\n",
      "[[ 139.40372335 -195.06661778  215.20327425  217.36083131]]\n",
      "[[ 139.4328396  -195.08506484  215.24243517  217.39765273]]\n",
      "[[ 139.46191722 -195.10348729  215.28154403  217.43442512]]\n",
      "[[ 139.4909563  -195.12188522  215.32060095  217.47114861]]\n",
      "[[ 139.51995696 -195.14025868  215.35960608  217.50782335]]\n",
      "The train loss is 0.000780954598857002\n",
      "The val loss is 0.0008538838657408209\n",
      "[[ 139.5489193  -195.15860774  215.39855957  217.54444946]]\n",
      "[[ 139.57784342 -195.17693248  215.43746155  217.58102708]]\n",
      "[[ 139.60672943 -195.19523296  215.47631217  217.61755635]]\n",
      "[[ 139.63557743 -195.21350926  215.51511157  217.65403739]]\n",
      "[[ 139.66438752 -195.23176143  215.55385988  217.69047035]]\n",
      "[[ 139.6931598  -195.24998955  215.59255726  217.72685535]]\n",
      "[[ 139.72189439 -195.26819369  215.63120383  217.76319252]]\n",
      "[[ 139.75059137 -195.28637391  215.66979974  217.79948201]]\n",
      "[[ 139.77925086 -195.30453027  215.70834513  217.83572393]]\n",
      "[[ 139.80787295 -195.32266285  215.74684012  217.87191842]]\n",
      "The train loss is 0.0007689312191993784\n",
      "The val loss is 0.0008407539838480404\n",
      "[[ 139.83645774 -195.34077172  215.78528487  217.9080656 ]]\n",
      "[[ 139.86500533 -195.35885693  215.8236795   217.94416561]]\n",
      "[[ 139.89351583 -195.37691855  215.86202416  217.98021858]]\n",
      "[[ 139.92198933 -195.39495665  215.90031897  218.01622463]]\n",
      "[[ 139.95042593 -195.4129713   215.93856407  218.0521839 ]]\n",
      "[[ 139.97882573 -195.43096256  215.9767596   218.0880965 ]]\n",
      "[[ 140.00718884 -195.44893048  216.0149057   218.12396256]]\n",
      "[[ 140.03551534 -195.46687515  216.05300248  218.15978222]]\n",
      "[[ 140.06380533 -195.48479662  216.09105009  218.1955556 ]]\n",
      "[[ 140.09205892 -195.50269495  216.12904866  218.23128282]]\n",
      "The train loss is 0.0007572741125919588\n",
      "The val loss is 0.000828023256670429\n",
      "[[ 140.12027619 -195.52057021  216.16699833  218.266964  ]]\n",
      "[[ 140.14845725 -195.53842246  216.20489921  218.30259928]]\n",
      "[[ 140.1766022  -195.55625176  216.24275145  218.33818877]]\n",
      "[[ 140.20471112 -195.57405819  216.28055517  218.37373259]]\n",
      "[[ 140.23278412 -195.59184179  216.3183105   218.40923088]]\n",
      "[[ 140.26082128 -195.60960263  216.35601758  218.44468375]]\n",
      "[[ 140.28882271 -195.62734078  216.39367652  218.48009133]]\n",
      "[[ 140.31678849 -195.64505629  216.43128747  218.51545373]]\n",
      "[[ 140.34471873 -195.66274923  216.46885054  218.55077108]]\n",
      "[[ 140.37261352 -195.68041965  216.50636587  218.58604349]]\n",
      "The train loss is 0.0007459665967924747\n",
      "The val loss is 0.0008156736219569827\n",
      "[[ 140.40047294 -195.69806762  216.54383358  218.6212711 ]]\n",
      "[[ 140.42829709 -195.71569321  216.58125379  218.656454  ]]\n",
      "[[ 140.45608607 -195.73329646  216.61862664  218.69159233]]\n",
      "[[ 140.48383997 -195.75087743  216.65595225  218.72668621]]\n",
      "[[ 140.51155888 -195.7684362   216.69323073  218.76173575]]\n",
      "[[ 140.53924288 -195.78597281  216.73046223  218.79674106]]\n",
      "[[ 140.56689208 -195.80348734  216.76764686  218.83170227]]\n",
      "[[ 140.59450656 -195.82097982  216.80478474  218.86661949]]\n",
      "[[ 140.62208642 -195.83845034  216.841876    218.90149285]]\n",
      "[[ 140.64963174 -195.85589893  216.87892076  218.93632244]]\n",
      "The train loss is 0.0007349930127492492\n",
      "The val loss is 0.0008036881040080394\n",
      "[[ 140.67714261 -195.87332567  216.91591914  218.97110839]]\n",
      "[[ 140.70461913 -195.89073061  216.95287126  219.00585082]]\n",
      "[[ 140.72974065 -195.90838177  216.98705611  219.03787901]]\n",
      "[[ 140.754831   -195.9260108   217.02119843  219.0698673 ]]\n",
      "[[ 140.77989026 -195.94361777  217.05529832  219.10181579]]\n",
      "[[ 140.80491852 -195.96120273  217.0893559   219.13372459]]\n",
      "[[ 140.82991584 -195.97876576  217.12337127  219.16559379]]\n",
      "[[ 140.85488231 -195.9963069   217.15734454  219.1974235 ]]\n",
      "[[ 140.879818   -196.01382622  217.19127583  219.22921382]]\n",
      "[[ 140.90472299 -196.03132377  217.22516524  219.26096486]]\n",
      "The train loss is 0.000724335114577691\n",
      "The val loss is 0.0007920507274965912\n",
      "[[ 140.92959736 -196.04879963  217.25901287  219.29267671]]\n",
      "[[ 140.95444118 -196.06625384  217.29281883  219.32434948]]\n",
      "[[ 140.97925454 -196.08368647  217.32658324  219.35598326]]\n",
      "[[ 141.0040375  -196.10109758  217.36030619  219.38757817]]\n",
      "[[ 141.02879014 -196.11848722  217.3939878   219.41913429]]\n",
      "[[ 141.05351254 -196.13585545  217.42762816  219.45065173]]\n",
      "[[ 141.07820478 -196.15320233  217.46122738  219.48213059]]\n",
      "[[ 141.10286693 -196.17052792  217.49478557  219.51357096]]\n",
      "[[ 141.12749906 -196.18783228  217.52830283  219.54497295]]\n",
      "[[ 141.15210125 -196.20511545  217.56177927  219.57633665]]\n",
      "The train loss is 0.00071398236701614\n",
      "The val loss is 0.0007807464467753861\n",
      "[[ 141.17667357 -196.22237751  217.59521498  219.60766216]]\n",
      "[[ 141.2012161  -196.2396185   217.62861008  219.63894958]]\n",
      "[[ 141.22572892 -196.25683849  217.66196466  219.67019901]]\n",
      "[[ 141.25021209 -196.27403752  217.69527883  219.70141054]]\n",
      "[[ 141.27466568 -196.29121566  217.72855269  219.73258426]]\n",
      "[[ 141.29908979 -196.30837295  217.76178634  219.76372028]]\n",
      "[[ 141.32348446 -196.32550946  217.79497988  219.79481869]]\n",
      "[[ 141.34784979 -196.34262524  217.82813341  219.82587959]]\n",
      "[[ 141.37218584 -196.35972034  217.86124704  219.85690307]]\n",
      "[[ 141.39649268 -196.37679482  217.89432086  219.88788922]]\n",
      "The train loss is 0.0007039231379508377\n",
      "The val loss is 0.0007697610916431658\n",
      "[[ 141.42077039 -196.39384873  217.92735498  219.91883813]]\n",
      "[[ 141.44501903 -196.41088213  217.96034948  219.94974992]]\n",
      "[[ 141.46923868 -196.42789507  217.99330448  219.98062465]]\n",
      "[[ 141.49342942 -196.4448876   218.02622007  220.01146244]]\n",
      "[[ 141.51759131 -196.46185978  218.05909635  220.04226337]]\n",
      "[[ 141.54172442 -196.47881166  218.09193342  220.07302753]]\n",
      "[[ 141.56582882 -196.49574329  218.12473137  220.10375502]]\n",
      "[[ 141.58990459 -196.51265472  218.1574903   220.13444593]]\n",
      "[[ 141.61395179 -196.52954601  218.19021031  220.16510035]]\n",
      "[[ 141.63797049 -196.5464172   218.2228915   220.19571837]]\n",
      "The train loss is 0.000694144839964263\n",
      "The val loss is 0.0007590812898726145\n",
      "[[ 141.66196077 -196.56326836  218.25553396  220.22630008]]\n",
      "[[ 141.6859227  -196.58009953  218.28813779  220.25684557]]\n",
      "[[ 141.70985633 -196.59691076  218.32070308  220.28735494]]\n",
      "[[ 141.73376175 -196.61370211  218.35322993  220.31782826]]\n",
      "[[ 141.75763902 -196.63047362  218.38571843  220.34826564]]\n",
      "[[ 141.78148821 -196.64722534  218.41816868  220.37866716]]\n",
      "[[ 141.80530939 -196.66395733  218.45058078  220.40903291]]\n",
      "[[ 141.82910262 -196.68066964  218.48295481  220.43936297]]\n",
      "[[ 141.85286798 -196.69736231  218.51529087  220.46965744]]\n",
      "[[ 141.87660553 -196.7140354   218.54758905  220.4999164 ]]\n",
      "The train loss is 0.0006846356420395315\n",
      "The val loss is 0.0007486944111587138\n",
      "[[ 141.90031533 -196.73068895  218.57984945  220.53013995]]\n",
      "[[ 141.92399747 -196.74732301  218.61207216  220.56032816]]\n",
      "[[ 141.947652   -196.76393764  218.64425727  220.59048113]]\n",
      "[[ 141.97127898 -196.78053288  218.67640487  220.62059893]]\n",
      "[[ 141.9948785  -196.79710878  218.70851505  220.65068167]]\n",
      "[[ 142.01845061 -196.81366539  218.74058791  220.68072942]]\n",
      "[[ 142.04199537 -196.83020276  218.77262354  220.71074227]]\n",
      "[[ 142.06551286 -196.84672093  218.80462202  220.7407203 ]]\n",
      "[[ 142.08900315 -196.86321995  218.83658345  220.7706636 ]]\n",
      "[[ 142.11246629 -196.87969988  218.86850792  220.80057225]]\n",
      "The train loss is 0.0006753844053637314\n",
      "The val loss is 0.0007385885161696942\n",
      "[[ 142.13590235 -196.89616075  218.90039551  220.83044634]]\n",
      "[[ 142.1593114  -196.91260262  218.93224632  220.86028596]]\n",
      "[[ 142.1826935  -196.92902553  218.96406044  220.89009118]]\n",
      "[[ 142.20604871 -196.94542953  218.99583795  220.91986209]]\n",
      "[[ 142.22937711 -196.96181467  219.02757894  220.94959877]]\n",
      "[[ 142.25267875 -196.97818099  219.0592835   220.9793013 ]]\n",
      "[[ 142.27595369 -196.99452853  219.09095171  221.00896978]]\n",
      "[[ 142.29920201 -197.01085735  219.12258368  221.03860427]]\n",
      "[[ 142.32242377 -197.0271675   219.15417947  221.06820486]]\n",
      "[[ 142.34561902 -197.04345901  219.18573918  221.09777164]]\n",
      "The train loss is 0.0006663806270150733\n",
      "The val loss is 0.0007287523097038323\n",
      "[[ 142.36878784 -197.05973193  219.2172629   221.12730468]]\n",
      "[[ 142.39193028 -197.0759863   219.24875071  221.15680407]]\n",
      "[[ 142.41504641 -197.09222218  219.2802027   221.18626988]]\n",
      "[[ 142.43813629 -197.10843961  219.31161895  221.2157022 ]]\n",
      "[[ 142.46119998 -197.12463863  219.34299955  221.24510111]]\n",
      "[[ 142.48423755 -197.14081929  219.37434458  221.27446669]]\n",
      "[[ 142.50724905 -197.15698162  219.40565413  221.30379901]]\n",
      "[[ 142.53023455 -197.17312569  219.43692828  221.33309815]]\n",
      "[[ 142.55319411 -197.18925152  219.46816712  221.3623642 ]]\n",
      "[[ 142.57612778 -197.20535917  219.49937073  221.39159724]]\n",
      "The train loss is 0.0006576143901850221\n",
      "The val loss is 0.0007191750975862044\n",
      "[[ 142.59903564 -197.22144867  219.53053919  221.42079733]]\n",
      "[[ 142.62191774 -197.23752008  219.56167259  221.44996457]]\n",
      "[[ 142.64477415 -197.25357342  219.59277101  221.47909902]]\n",
      "[[ 142.66760491 -197.26960876  219.62383453  221.50820077]]\n",
      "[[ 142.6904101  -197.28562613  219.65486324  221.53726989]]\n",
      "[[ 142.71318977 -197.30162557  219.68585721  221.56630646]]\n",
      "[[ 142.73594399 -197.31760713  219.71681653  221.59531056]]\n",
      "[[ 142.7586728  -197.33357084  219.74774129  221.62428227]]\n",
      "[[ 142.78137628 -197.34951676  219.77863155  221.65322165]]\n",
      "[[ 142.80405448 -197.36544492  219.8094874   221.68212878]]\n",
      "The train loss is 0.0006490763198814273\n",
      "The val loss is 0.0007098467469752086\n",
      "[[ 142.82670746 -197.38135537  219.84030893  221.71100375]]\n",
      "[[ 142.84933527 -197.39724814  219.87109621  221.73984663]]\n",
      "[[ 142.87193799 -197.41312329  219.90184932  221.76865749]]\n",
      "[[ 142.89451566 -197.42898084  219.93256835  221.7974364 ]]\n",
      "[[ 142.91706834 -197.44482085  219.96325337  221.82618344]]\n",
      "[[ 142.93959609 -197.46064335  219.99390446  221.8548987 ]]\n",
      "[[ 142.96209897 -197.47644839  220.0245217   221.88358223]]\n",
      "[[ 142.98457704 -197.492236    220.05510517  221.91223411]]\n",
      "[[ 143.00703035 -197.50800623  220.08565495  221.94085443]]\n",
      "[[ 143.02945897 -197.52375912  220.11617111  221.96944324]]\n",
      "The train loss is 0.0006407575432740386\n",
      "The val loss is 0.000700757649775275\n",
      "[[ 143.05186294 -197.53949471  220.14665374  221.99800063]]\n",
      "[[ 143.07424233 -197.55521303  220.17710291  222.02652667]]\n",
      "[[ 143.09659719 -197.57091413  220.20751869  222.05502142]]\n",
      "[[ 143.11892757 -197.58659806  220.23790118  222.08348497]]\n",
      "[[ 143.14123355 -197.60226484  220.26825043  222.11191738]]\n",
      "[[ 143.16351516 -197.61791452  220.29856654  222.14031873]]\n",
      "[[ 143.18577247 -197.63354713  220.32884957  222.16868908]]\n",
      "[[ 143.20800553 -197.64916273  220.3590996   222.19702852]]\n",
      "[[ 143.23021439 -197.66476134  220.38931671  222.2253371 ]]\n",
      "[[ 143.25239912 -197.68034301  220.41950097  222.25361491]]\n",
      "The train loss is 0.0006326496540098433\n",
      "The val loss is 0.0006918986888800236\n",
      "[[ 143.27455977 -197.69590778  220.44965246  222.281862  ]]\n",
      "[[ 143.29669639 -197.71145568  220.47977125  222.31007846]]\n",
      "[[ 143.31880904 -197.72698675  220.50985742  222.33826435]]\n",
      "[[ 143.34089777 -197.74250104  220.53991104  222.36641974]]\n",
      "[[ 143.36296263 -197.75799857  220.56993219  222.39454471]]\n",
      "[[ 143.38500369 -197.7734794   220.59992094  222.42263931]]\n",
      "[[ 143.40702099 -197.78894355  220.62987736  222.45070363]]\n",
      "[[ 143.42901459 -197.80439107  220.65980153  222.47873772]]\n",
      "[[ 143.45098454 -197.81982199  220.68969352  222.50674166]]\n",
      "[[ 143.4729309  -197.83523635  220.7195534   222.53471552]]\n",
      "The train loss is 0.0006247446799536003\n",
      "The val loss is 0.000683261206998621\n",
      "[[ 143.49485371 -197.85063419  220.74938125  222.56265937]]\n",
      "[[ 143.51675304 -197.86601554  220.77917714  222.59057326]]\n",
      "[[ 143.53862893 -197.88138045  220.80894114  222.61845728]]\n",
      "[[ 143.56048144 -197.89672896  220.83867333  222.64631148]]\n",
      "[[ 143.58231062 -197.91206109  220.86837377  222.67413594]]\n",
      "[[ 143.60411653 -197.92737688  220.89804253  222.70193072]]\n",
      "[[ 143.62589921 -197.94267638  220.92767969  222.72969589]]\n",
      "[[ 143.64765871 -197.95795961  220.95728532  222.75743152]]\n",
      "[[ 143.66939509 -197.97322662  220.98685949  222.78513767]]\n",
      "[[ 143.69110841 -197.98847745  221.01640227  222.81281441]]\n",
      "The train loss is 0.0006170350539042523\n",
      "The val loss is 0.0006748369778373468\n",
      "[[ 143.71279871 -198.00371212  221.04591374  222.8404618 ]]\n",
      "[[ 143.73446604 -198.01893067  221.07539395  222.86807991]]\n",
      "[[ 143.75611045 -198.03413315  221.10484298  222.89566881]]\n",
      "[[ 143.777732   -198.04931959  221.1342609   222.92322856]]\n",
      "[[ 143.79933074 -198.06449001  221.16364778  222.95075922]]\n",
      "[[ 143.82090672 -198.07964447  221.19300369  222.97826087]]\n",
      "[[ 143.84245998 -198.09478299  221.22232869  223.00573356]]\n",
      "[[ 143.86399058 -198.10990561  221.25162287  223.03317736]]\n",
      "[[ 143.88549857 -198.12501236  221.28088627  223.06059233]]\n",
      "[[ 143.906984   -198.14010329  221.31011898  223.08797855]]\n",
      "The train loss is 0.0006095135869167356\n",
      "The val loss is 0.0006666181794345495\n",
      "[[ 143.92844692 -198.15517842  221.33932106  223.11533606]]\n",
      "[[ 143.94988737 -198.17023779  221.36849258  223.14266494]]\n",
      "[[ 143.97130542 -198.18528144  221.3976336   223.16996525]]\n",
      "[[ 143.9927011  -198.20030939  221.4267442   223.19723705]]\n",
      "[[ 144.01407447 -198.2153217   221.45582444  223.22448041]]\n",
      "[[ 144.03542557 -198.23031838  221.48487438  223.25169538]]\n",
      "[[ 144.05675445 -198.24529947  221.5138941   223.27888204]]\n",
      "[[ 144.07806118 -198.26026502  221.54288366  223.30604043]]\n",
      "[[ 144.09934578 -198.27521504  221.57184312  223.33317063]]\n",
      "[[ 144.12060831 -198.29014958  221.60077256  223.3602727 ]]\n",
      "The train loss is 0.0006021734439176572\n",
      "The val loss is 0.0006585973694615319\n",
      "[[ 144.14184882 -198.30506868  221.62967204  223.38734669]]\n",
      "[[ 144.16306736 -198.31997235  221.65854162  223.41439268]]\n",
      "[[ 144.18426397 -198.33486065  221.68738137  223.44141072]]\n",
      "[[ 144.20543871 -198.34973359  221.71619135  223.46840086]]\n",
      "[[ 144.22659161 -198.36459122  221.74497163  223.49536318]]\n",
      "[[ 144.24772273 -198.37943357  221.77372228  223.52229774]]\n",
      "[[ 144.26883212 -198.39426068  221.80244336  223.54920459]]\n",
      "[[ 144.28991982 -198.40907256  221.83113492  223.57608379]]\n",
      "[[ 144.31098588 -198.42386927  221.85979705  223.6029354 ]]\n",
      "[[ 144.33203034 -198.43865082  221.88842979  223.6297595 ]]\n",
      "The train loss is 0.0005950081213539537\n",
      "The val loss is 0.0006507674623245144\n",
      "[[ 144.35305326 -198.45341726  221.91703322  223.65655612]]\n",
      "[[ 144.37405467 -198.46816862  221.9456074   223.68332534]]\n",
      "[[ 144.39503464 -198.48290493  221.97415239  223.71006721]]\n",
      "[[ 144.41599319 -198.49762621  222.00266825  223.73678179]]\n",
      "[[ 144.43693038 -198.51233252  222.03115505  223.76346915]]\n",
      "[[ 144.45784626 -198.52702386  222.05961285  223.79012933]]\n",
      "[[ 144.47874087 -198.54170029  222.08804172  223.8167624 ]]\n",
      "[[ 144.49961425 -198.55636183  222.1164417   223.84336842]]\n",
      "[[ 144.52046645 -198.57100851  222.14481287  223.86994744]]\n",
      "[[ 144.54129752 -198.58564036  222.17315529  223.89649952]]\n",
      "The train loss is 0.0005880114266500699\n",
      "The val loss is 0.0006431217079141859\n",
      "[[ 144.56210751 -198.60025742  222.20146902  223.92302473]]\n",
      "[[ 144.58289645 -198.61485972  222.22975413  223.94952311]]\n",
      "[[ 144.60366439 -198.62944729  222.25801066  223.97599473]]\n",
      "[[ 144.62441137 -198.64402016  222.28623868  224.00243963]]\n",
      "[[ 144.64513745 -198.65857836  222.31443827  224.02885789]]\n",
      "[[ 144.66584267 -198.67312193  222.34260946  224.05524956]]\n",
      "[[ 144.68652707 -198.68765089  222.37075233  224.08161469]]\n",
      "[[ 144.70719069 -198.70216527  222.39886694  224.10795333]]\n",
      "[[ 144.72783358 -198.71666512  222.42695334  224.13426556]]\n",
      "[[ 144.74845578 -198.73115045  222.4550116   224.16055141]]\n",
      "The train loss is 0.0005811774592844835\n",
      "The val loss is 0.000635653671867313\n",
      "[[ 144.76905734 -198.7456213   222.48304177  224.18681095]]\n",
      "[[ 144.7896383  -198.76007769  222.51104392  224.21304424]]\n",
      "[[ 144.8101987  -198.77451967  222.5390181   224.23925132]]\n",
      "[[ 144.83073859 -198.78894726  222.56696437  224.26543226]]\n",
      "[[ 144.85125801 -198.80336049  222.5948828   224.29158711]]\n",
      "[[ 144.871757   -198.81775938  222.62277343  224.31771593]]\n",
      "[[ 144.89223561 -198.83214398  222.65063634  224.34381876]]\n",
      "[[ 144.91269388 -198.84651431  222.67847157  224.36989567]]\n",
      "[[ 144.93313186 -198.8608704   222.70627919  224.3959467 ]]\n",
      "[[ 144.95354957 -198.87521228  222.73405925  224.42197192]]\n",
      "The train loss is 0.0005745005933196033\n",
      "The val loss is 0.0006283572172143949\n",
      "[[ 144.97394708 -198.88953999  222.76181182  224.44797138]]\n",
      "[[ 144.99432441 -198.90385354  222.78953694  224.47394513]]\n",
      "[[ 145.01468162 -198.91815297  222.81723468  224.49989322]]\n",
      "[[ 145.03501874 -198.93243831  222.84490509  224.52581571]]\n",
      "[[ 145.05533581 -198.94670959  222.87254823  224.55171266]]\n",
      "[[ 145.07563288 -198.96096684  222.90016416  224.57758411]]\n",
      "[[ 145.09591    -198.97521008  222.92775294  224.60343012]]\n",
      "[[ 145.11616719 -198.98943935  222.95531462  224.62925074]]\n",
      "[[ 145.1364045  -199.00365467  222.98284925  224.65504602]]\n",
      "[[ 145.15662198 -199.01785608  223.0103569   224.68081602]]\n",
      "The train loss is 0.0005679754612417451\n",
      "The val loss is 0.0006212264873017076\n",
      "[[ 145.17681967 -199.03204361  223.03783761  224.7065608 ]]\n",
      "[[ 145.19699759 -199.04621727  223.06529145  224.73228039]]\n",
      "[[ 145.21715581 -199.0603771   223.09271848  224.75797486]]\n",
      "[[ 145.23729435 -199.07452314  223.12011874  224.78364425]]\n",
      "[[ 145.25741326 -199.0886554   223.14749229  224.80928862]]\n",
      "[[ 145.27751258 -199.10277391  223.17483918  224.83490803]]\n",
      "[[ 145.29759235 -199.11687871  223.20215948  224.86050251]]\n",
      "[[ 145.31765261 -199.13096983  223.22945324  224.88607212]]\n",
      "[[ 145.33769339 -199.14504728  223.2567205   224.91161692]]\n",
      "[[ 145.35771475 -199.1591111   223.28396134  224.93713696]]\n",
      "The train loss is 0.0005615969389886627\n",
      "The val loss is 0.0006142558898843902\n",
      "[[ 145.37771671 -199.17316132  223.31117579  224.96263227]]\n",
      "[[ 145.39769932 -199.18719796  223.33836392  224.98810293]]\n",
      "[[ 145.41766262 -199.20122105  223.36552577  225.01354897]]\n",
      "[[ 145.43760665 -199.21523062  223.3926614   225.03897045]]\n",
      "[[ 145.45753145 -199.2292267   223.41977088  225.06436742]]\n",
      "[[ 145.47743705 -199.24320932  223.44685423  225.08973992]]\n",
      "[[ 145.4973235  -199.2571785   223.47391154  225.11508801]]\n",
      "[[ 145.51719084 -199.27113426  223.50094283  225.14041174]]\n",
      "[[ 145.53703909 -199.28507664  223.52794818  225.16571116]]\n",
      "[[ 145.55686831 -199.29900567  223.55492762  225.19098631]]\n",
      "The train loss is 0.0005553601320511349\n",
      "The val loss is 0.0006074400822974894\n",
      "[[ 145.57667854 -199.31292137  223.58188122  225.21623725]]\n",
      "[[ 145.5964698  -199.32682377  223.60880903  225.24146402]]\n",
      "[[ 145.61624214 -199.34071289  223.63571109  225.26666668]]\n",
      "[[ 145.63599559 -199.35458876  223.66258746  225.29184527]]\n",
      "[[ 145.6557302  -199.36845141  223.6894382   225.31699984]]\n",
      "[[ 145.675446   -199.38230086  223.71626335  225.34213044]]\n",
      "[[ 145.69514304 -199.39613715  223.74306296  225.36723711]]\n",
      "[[ 145.71482134 -199.4099603   223.76983709  225.39231992]]\n",
      "[[ 145.73448095 -199.42377033  223.79658579  225.4173789 ]]\n",
      "[[ 145.7541219  -199.43756727  223.82330912  225.4424141 ]]\n",
      "The train loss is 0.000549260362555987\n",
      "The val loss is 0.0006007739576209083\n",
      "[[ 145.77374424 -199.45135115  223.85000711  225.46742557]]\n",
      "[[ 145.79334799 -199.46512199  223.87667982  225.49241336]]\n",
      "[[ 145.8129332  -199.47887982  223.90332731  225.51737752]]\n",
      "[[ 145.8324999  -199.49262467  223.92994961  225.54231809]]\n",
      "[[ 145.85204813 -199.50635656  223.9565468   225.56723512]]\n",
      "[[ 145.87157793 -199.52007551  223.9831189   225.59212865]]\n",
      "[[ 145.89108934 -199.53378156  224.00966598  225.61699874]]\n",
      "[[ 145.91058238 -199.54747473  224.03618808  225.64184544]]\n",
      "[[ 145.93005711 -199.56115504  224.06268526  225.66666878]]\n",
      "[[ 145.94951354 -199.57482252  224.08915756  225.69146881]]\n",
      "The train loss is 0.0005432931572424852\n",
      "The val loss is 0.0005942526317593264\n",
      "[[ 145.96895173 -199.5884772   224.11560503  225.71624558]]\n",
      "[[ 145.9883717  -199.6021191   224.14202772  225.74099915]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 146.0077735  -199.61574825  224.16842569  225.76572954]]\n",
      "[[ 146.02715715 -199.62936466  224.19479897  225.79043681]]\n",
      "[[ 146.0465227  -199.64296837  224.22114763  225.81512101]]\n",
      "[[ 146.06587018 -199.65655941  224.2474717   225.83978218]]\n",
      "[[ 146.08519963 -199.67013779  224.27377124  225.86442037]]\n",
      "[[ 146.10451108 -199.68370354  224.30004629  225.88903561]]\n",
      "[[ 146.12380456 -199.69725669  224.32629691  225.91362796]]\n",
      "[[ 146.14308012 -199.71079726  224.35252314  225.93819747]]\n",
      "The train loss is 0.0005374542362557888\n",
      "The val loss is 0.0005878714313687156\n",
      "[[ 146.16233779 -199.72432528  224.37872503  225.96274417]]\n",
      "[[ 146.1815776  -199.73784077  224.40490262  225.98726811]]\n",
      "[[ 146.20079959 -199.75134375  224.43105597  226.01176934]]\n",
      "[[ 146.22000379 -199.76483425  224.45718512  226.0362479 ]]\n",
      "[[ 146.23919024 -199.7783123   224.48329013  226.06070384]]\n",
      "[[ 146.25835897 -199.79177791  224.50937103  226.0851372 ]]\n",
      "[[ 146.27751003 -199.80523112  224.53542787  226.10954802]]\n",
      "[[ 146.29664343 -199.81867195  224.56146071  226.13393635]]\n",
      "[[ 146.31575922 -199.83210041  224.58746959  226.15830223]]\n",
      "[[ 146.33485743 -199.84551654  224.61345455  226.1826457 ]]\n",
      "The train loss is 0.0005317395026905797\n",
      "The val loss is 0.000581625882562933\n",
      "[[ 146.3539381  -199.85892036  224.63941564  226.20696682]]\n",
      "[[ 146.37300126 -199.8723119   224.66535291  226.23126562]]\n",
      "[[ 146.39204694 -199.88569117  224.69126641  226.25554214]]\n",
      "[[ 146.41107518 -199.8990582   224.71715618  226.27979644]]\n",
      "[[ 146.43008601 -199.91241302  224.74302226  226.30402855]]\n",
      "[[ 146.44907946 -199.92575564  224.76886471  226.32823851]]\n",
      "[[ 146.46805558 -199.93908609  224.79468356  226.35242638]]\n",
      "[[ 146.48701439 -199.9524044   224.82047888  226.37659218]]\n",
      "[[ 146.50595592 -199.96571059  224.84625069  226.40073597]]\n",
      "[[ 146.52488021 -199.97900468  224.87199904  226.42485779]]\n",
      "The train loss is 0.0005261450328227894\n",
      "The val loss is 0.0005755117003442557\n",
      "[[ 146.5437873  -199.9922867   224.89772399  226.44895768]]\n",
      "[[ 146.56267721 -200.00555667  224.92342557  226.47303567]]\n",
      "[[ 146.58154998 -200.0188146   224.94910384  226.49709183]]\n",
      "[[ 146.60040564 -200.03206053  224.97475883  226.52112617]]\n",
      "[[ 146.61924423 -200.04529448  225.00039058  226.54513876]]\n",
      "[[ 146.63806578 -200.05851647  225.02599916  226.56912962]]\n",
      "[[ 146.65687031 -200.07172652  225.05158459  226.59309881]]\n",
      "[[ 146.67565787 -200.08492466  225.07714693  226.61704636]]\n",
      "[[ 146.69442849 -200.09811091  225.10268622  226.64097231]]\n",
      "[[ 146.71318219 -200.11128529  225.1282025   226.66487671]]\n",
      "The train loss is 0.0005206670669744284\n",
      "The val loss is 0.0005695247787014455\n",
      "[[ 146.73191901 -200.12444783  225.15369581  226.6887596 ]]\n",
      "[[ 146.75063899 -200.13759854  225.17916621  226.71262101]]\n",
      "[[ 146.76934215 -200.15073745  225.20461372  226.73646099]]\n",
      "[[ 146.78802852 -200.16386458  225.23003841  226.76027958]]\n",
      "[[ 146.80669815 -200.17697996  225.2554403   226.78407683]]\n",
      "[[ 146.82535105 -200.19008361  225.28081945  226.80785276]]\n",
      "[[ 146.84398727 -200.20317554  225.3061759   226.83160742]]\n",
      "[[ 146.86260683 -200.21625578  225.33150969  226.85534086]]\n",
      "[[ 146.88120977 -200.22932436  225.35682086  226.87905311]]\n",
      "[[ 146.89979611 -200.24238129  225.38210946  226.90274421]]\n",
      "The train loss is 0.0005153020009634808\n",
      "The val loss is 0.0005636611813281574\n",
      "[[ 146.91836589 -200.2554266   225.40737552  226.9264142 ]]\n",
      "[[ 146.93691914 -200.26846031  225.4326191   226.95006312]]\n",
      "[[ 146.95545589 -200.28148244  225.45784023  226.97369102]]\n",
      "[[ 146.97397618 -200.29449301  225.48303896  226.99729793]]\n",
      "[[ 146.99248002 -200.30749205  225.50821532  227.02088388]]\n",
      "[[ 147.01096746 -200.32047957  225.53336937  227.04444893]]\n",
      "[[ 147.02943853 -200.3334556   225.55850114  227.06799311]]\n",
      "[[ 147.04789325 -200.34642016  225.58361067  227.09151646]]\n",
      "[[ 147.06633166 -200.35937327  225.60869801  227.11501901]]\n",
      "[[ 147.08475378 -200.37231496  225.63376319  227.13850082]]\n",
      "The train loss is 0.0005100463780912807\n",
      "The val loss is 0.0005579171329153534\n",
      "[[ 147.10315965 -200.38524523  225.65880627  227.1619619 ]]\n",
      "[[ 147.1215493  -200.39816412  225.68382727  227.18540232]]\n",
      "[[ 147.13992276 -200.41107165  225.70882625  227.20882209]]\n",
      "[[ 147.15828005 -200.42396783  225.73380324  227.23222127]]\n",
      "[[ 147.17662122 -200.43685269  225.75875829  227.25559989]]\n",
      "[[ 147.19494628 -200.44972626  225.78369143  227.27895799]]\n",
      "[[ 147.21325527 -200.46258854  225.8086027   227.3022956 ]]\n",
      "[[ 147.23154822 -200.47543956  225.83349215  227.32561277]]\n",
      "[[ 147.24982516 -200.48827935  225.85835982  227.34890953]]\n",
      "[[ 147.26808612 -200.50110792  225.88320574  227.37218592]]\n",
      "The train loss is 0.0005048968816286192\n",
      "The val loss is 0.0005522890109778958\n",
      "[[ 147.28633112 -200.51392529  225.90802997  227.39544199]]\n",
      "[[ 147.3045602  -200.52673148  225.93283253  227.41867775]]\n",
      "[[ 147.32277339 -200.53952653  225.95761347  227.44189327]]\n",
      "[[ 147.34097071 -200.55231043  225.98237282  227.46508856]]\n",
      "[[ 147.3591522  -200.56508323  226.00711064  227.48826367]]\n",
      "[[ 147.37731788 -200.57784493  226.03182695  227.51141864]]\n",
      "[[ 147.39546779 -200.59059556  226.0565218   227.5345535 ]]\n",
      "[[ 147.41360195 -200.60333513  226.08119522  227.55766829]]\n",
      "[[ 147.4317204  -200.61606367  226.10584727  227.58076305]]\n"
     ]
    }
   ],
   "source": [
    "nueralnet=MultiLayerNueralNet(x_train.shape[0],1,1)\n",
    "nueralnet.forward_propgation(x_train)\n",
    "nueralnet.backward_propagation(x_train,y_train)\n",
    "train_loss,test_loss=nueralnet.train(x_train,y_train,x_test,y_test,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(train_loss)\n",
    "plt.plot(test_loss)\n",
    "plt.ylabel('loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "100.0\n"
     ]
    }
   ],
   "source": [
    "feature=np.array([-1.14452391, -1.19360307])\n",
    "print(nueralnet.accuracy(x_test,y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 3.10997534e-01, -1.73673948e-01,  2.24968346e+00,\n",
       "         1.89829664e-01,  1.15917263e+00, -5.37177559e-01,\n",
       "        -2.94841818e-01,  1.28034050e+00,  4.32165405e-01,\n",
       "        -5.25060772e-02,  7.95669016e-01, -1.26418478e+00,\n",
       "        -4.16009689e-01, -1.14301691e+00, -9.00681170e-01,\n",
       "         5.53333275e-01,  7.95669016e-01, -2.94841818e-01,\n",
       "        -1.73673948e-01,  6.74501145e-01, -1.38535265e+00,\n",
       "         3.10997534e-01, -1.02184904e+00,  6.74501145e-01,\n",
       "         2.49201920e+00,  1.03800476e+00,  1.03800476e+00,\n",
       "         1.15917263e+00, -1.26418478e+00, -1.26418478e+00,\n",
       "        -1.50652052e+00, -1.73673948e-01,  1.03800476e+00,\n",
       "        -1.26418478e+00, -1.74885626e+00,  5.53333275e-01,\n",
       "         6.74501145e-01, -7.79513300e-01, -1.02184904e+00,\n",
       "        -7.79513300e-01, -5.25060772e-02,  1.89829664e-01,\n",
       "         1.03800476e+00, -5.37177559e-01, -5.37177559e-01,\n",
       "        -4.16009689e-01,  5.53333275e-01,  6.74501145e-01,\n",
       "         9.16836886e-01,  1.64384411e+00],\n",
       "       [-5.92373012e-01,  1.70959465e+00, -1.05276654e+00,\n",
       "        -3.62176246e-01, -5.92373012e-01,  7.88807586e-01,\n",
       "        -3.62176246e-01,  9.82172869e-02, -1.97355361e+00,\n",
       "        -8.22569778e-01,  3.28414053e-01, -1.31979479e-01,\n",
       "         1.01900435e+00,  9.82172869e-02,  1.70959465e+00,\n",
       "         5.58610819e-01, -1.31979479e-01, -1.28296331e+00,\n",
       "        -5.92373012e-01, -5.92373012e-01,  3.28414053e-01,\n",
       "        -1.31979479e-01,  7.88807586e-01, -5.92373012e-01,\n",
       "         1.70959465e+00, -1.31979479e-01, -1.28296331e+00,\n",
       "         3.28414053e-01, -1.31979479e-01,  9.82172869e-02,\n",
       "         1.24920112e+00,  3.09077525e+00,  9.82172869e-02,\n",
       "         7.88807586e-01,  3.28414053e-01, -1.28296331e+00,\n",
       "         3.28414053e-01,  1.01900435e+00,  1.24920112e+00,\n",
       "         2.40018495e+00, -8.22569778e-01,  7.88807586e-01,\n",
       "         9.82172869e-02,  1.93979142e+00,  1.47939788e+00,\n",
       "        -1.51316008e+00, -5.92373012e-01,  9.82172869e-02,\n",
       "        -1.31979479e-01,  1.24920112e+00],\n",
       "       [ 5.35408562e-01, -1.16971425e+00,  1.78583195e+00,\n",
       "         4.21733708e-01,  5.92245988e-01, -1.28338910e+00,\n",
       "        -8.98031345e-02,  7.62758269e-01,  4.21733708e-01,\n",
       "         8.07091462e-02,  7.62758269e-01, -1.34022653e+00,\n",
       "        -1.39706395e+00, -1.28338910e+00, -1.28338910e+00,\n",
       "         5.35408562e-01,  1.16062026e+00,  8.07091462e-02,\n",
       "         4.21733708e-01,  1.04694540e+00, -1.22655167e+00,\n",
       "         6.49083415e-01, -1.22655167e+00,  1.04694540e+00,\n",
       "         1.50164482e+00,  8.19595696e-01,  1.16062026e+00,\n",
       "         1.21745768e+00, -1.34022653e+00, -1.22655167e+00,\n",
       "        -1.56757623e+00, -1.28338910e+00,  3.64896281e-01,\n",
       "        -1.22655167e+00, -1.39706395e+00,  7.05920842e-01,\n",
       "         4.21733708e-01, -1.28338910e+00, -1.34022653e+00,\n",
       "        -1.28338910e+00,  7.62758269e-01,  4.21733708e-01,\n",
       "         5.35408562e-01, -1.39706395e+00, -1.28338910e+00,\n",
       "        -3.29657076e-02,  7.62758269e-01,  9.90107977e-01,\n",
       "         3.64896281e-01,  1.33113254e+00],\n",
       "       [ 8.77547895e-04, -1.18381211e+00,  1.44883158e+00,\n",
       "         3.95774101e-01,  2.64141916e-01, -1.05217993e+00,\n",
       "         1.32509732e-01,  1.44883158e+00,  3.95774101e-01,\n",
       "         8.77547895e-04,  1.05393502e+00, -1.44707648e+00,\n",
       "        -1.31544430e+00, -1.44707648e+00, -1.18381211e+00,\n",
       "         5.27406285e-01,  1.31719939e+00, -1.30754636e-01,\n",
       "         1.32509732e-01,  1.31719939e+00, -1.31544430e+00,\n",
       "         7.90670654e-01, -1.05217993e+00,  1.18556721e+00,\n",
       "         1.05393502e+00,  1.44883158e+00,  7.90670654e-01,\n",
       "         1.44883158e+00, -1.18381211e+00, -1.31544430e+00,\n",
       "        -1.31544430e+00, -1.05217993e+00,  2.64141916e-01,\n",
       "        -1.31544430e+00, -1.31544430e+00,  9.22302838e-01,\n",
       "         3.95774101e-01, -1.31544430e+00, -1.31544430e+00,\n",
       "        -1.44707648e+00,  9.22302838e-01,  5.27406285e-01,\n",
       "         3.95774101e-01, -1.05217993e+00, -1.31544430e+00,\n",
       "        -2.62386821e-01,  3.95774101e-01,  7.90670654e-01,\n",
       "         2.64141916e-01,  1.71209594e+00]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_test"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

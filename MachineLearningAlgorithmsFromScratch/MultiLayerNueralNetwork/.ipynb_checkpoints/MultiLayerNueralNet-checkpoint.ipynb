{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "importing Jupyter notebook from HelperFunctions.ipynb\n",
      "Sigmoid at 0 is 0.5\n",
      "relu at 0 is 0\n",
      "relu at -ve is 0\n",
      "relu at +ve is [[0 0 1 2]\n",
      " [0 0 0 0]]\n",
      "-0.19129833709962918\n",
      "-0.19129833709962912\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import sklearn\n",
    "import sklearn.datasets\n",
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "import sklearn.linear_model\n",
    "import sklearn.linear_model\n",
    "from sklearn.model_selection import train_test_split\n",
    "import import_ipynb\n",
    "from HelperFunctions import relu,sigmoid,loss\n",
    "np.random.seed(1) # set a seed so that the results are consistent\n",
    "#from ipynb.fs.full.HelperFunctions import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.datasets.samples_generator import make_blobs\n",
    "from matplotlib import pyplot\n",
    "from pandas import DataFrame\n",
    "# generate 2d classification dataset\n",
    "X, Y = make_blobs(n_samples=200, centers=2, n_features=2)\n",
    "# scatter plot, dots colored by class value\n",
    "df = DataFrame(dict(x=X[:,0], y=X[:,1], label=Y))\n",
    "colors = {0:'red', 1:'blue', 2:'green'}\n",
    "fig, ax = pyplot.subplots()\n",
    "grouped = df.groupby('label')\n",
    "for key, group in grouped:\n",
    "    group.plot(ax=ax, kind='scatter', x='x', y='y', label=key, color=colors[key])\n",
    "pyplot.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 0, 1, 1, 0, 1, 0, 1, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 1,\n",
       "       1, 0, 0, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, 0, 1, 1, 0, 0, 1, 0,\n",
       "       1, 1, 0, 1, 0, 0, 1, 1, 0, 0, 0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1,\n",
       "       1, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, 0, 1, 0, 0,\n",
       "       0, 0, 1, 0, 1, 0, 0, 1, 0, 1, 1, 0, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1,\n",
       "       0, 0, 0, 1, 0, 1, 1, 1, 0, 1, 1, 1, 0, 0, 0, 1, 1, 1, 0, 1, 0, 0,\n",
       "       1, 1, 1, 0, 0, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 0, 0, 0, 1, 0,\n",
       "       0, 0, 0, 0, 0, 1, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 1,\n",
       "       0, 0, 0, 1, 0, 1, 0, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 1, 1, 1, 0, 0,\n",
       "       1, 0])"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#data_to_test the algorithm\n",
    "'''from sklearn.datasets import load_iris\n",
    "data=load_iris()\n",
    "X=data.data\n",
    "Y=data.target\n",
    "Y[Y>0]=1'''\n",
    "feature_mean=np.mean(X,axis=0)\n",
    "feature_SD=np.std(X,axis=0)\n",
    "X=(X-feature_mean)/feature_SD\n",
    "X\n",
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(134, 2)\n",
      "(66, 2)\n",
      "(134,)\n",
      "(66,)\n"
     ]
    }
   ],
   "source": [
    "Y.reshape((Y.shape[0],1))\n",
    "x_train,x_test, y_train, y_test = train_test_split(X, Y, test_size=0.33, random_state=42)\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2, 134)\n",
      "(2, 66)\n",
      "(1, 134)\n",
      "(1, 66)\n"
     ]
    }
   ],
   "source": [
    "x_train=x_train.T\n",
    "x_test=x_test.T\n",
    "y_train=y_train.reshape((y_train.shape[0],1))\n",
    "y_train=y_train.T\n",
    "y_test=y_test.reshape((y_test.shape[0],1))\n",
    "y_test=y_test.T\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Intialize Model Parameters\n",
    "\n",
    "class MultiLayerNueralNet():\n",
    "    \n",
    "    \n",
    "    def __init__(self,num_input,num_hidden,num_output):\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        intialize class  instance variables \n",
    "        Argument:\n",
    "        num_input -- size of the input layer\n",
    "        num_hidden -- size of the hidden layer\n",
    "        num_ouput -- size of the output layer\n",
    "        \n",
    "        \"\"\"\n",
    "        \n",
    "        self.num_input=num_input\n",
    "        self.num_hidden=num_hidden\n",
    "        self.num_output=num_output\n",
    "        self.W1=np.random.rand(num_hidden,num_input)*0.01\n",
    "        self.W2=np.random.rand(num_output,num_hidden)*0.01\n",
    "        self.b1=np.zeros((num_hidden,1))\n",
    "        self.b2=np.zeros((num_output,1))\n",
    "        self.cache={\"Z1\": None,\"A1\": None,\"Z2\": None,\"A2\": None}\n",
    "        self.grad_cache={\"dW1\": None,\"dW2\": None,\"db1\": None,\"db2\": None}\n",
    "        \n",
    "   \n",
    "    \n",
    "    def forward_propgation(self,X,Y):\n",
    "        \n",
    "        m=X.shape[1]\n",
    "        W1=self.W1\n",
    "        W2=self.W2\n",
    "        b1=self.b1\n",
    "        b2=self.b2\n",
    "        Z1=np.dot(W1,X)+b1\n",
    "        A1=sigmoid(Z1)\n",
    "        Z2=np.dot(W2,A1)+b2\n",
    "        A2=sigmoid(Z2)\n",
    "        self.cache[\"Z1\"]=Z1\n",
    "        self.cache[\"A1\"]=A1\n",
    "        self.cache[\"Z2\"]=Z2\n",
    "        self.cache[\"A2\"]=A2\n",
    "     \n",
    "    \n",
    "    def backward_propagation(self,X,Y):\n",
    "    \n",
    "        #one_hot_y=np.eye(3)[Y]\n",
    "        m=X.shape[1]\n",
    "        W1=self.W1\n",
    "        W2=self.W2\n",
    "        b1=self.b1\n",
    "        b2=self.b2\n",
    "        Z1=self.cache[\"Z1\"]\n",
    "        A1=self.cache[\"A1\"]\n",
    "        Z2=self.cache[\"Z2\"]\n",
    "        A2=self.cache[\"A2\"]\n",
    "    \n",
    "        dZ2=A2 - Y\n",
    "        dW2=(1./m)*np.dot(dZ2,A1.T)\n",
    "        db2=(1./m)*np.sum(dZ2,axis=1, keepdims = True)\n",
    "        #dZ1=np.dot(W2.T,dZ2)*A1*(1-A1)\n",
    "        dZ1=np.multiply(np.dot(W2.T,dZ2),np.int64(A1 > 0))\n",
    "        dW1=(1./m)*np.dot(dZ1,X.T)\n",
    "        db1=(1./m)*np.sum(dZ1,axis=1, keepdims = True)\n",
    "        self.grad_cache[\"dW1\"]=dW1\n",
    "        self.grad_cache[\"dW2\"]=dW2\n",
    "        self.grad_cache[\"db1\"]=db1\n",
    "        self.grad_cache[\"db2\"]=db2\n",
    "        \n",
    "        \n",
    "    #def gradient_check():\n",
    "        \n",
    "    \n",
    "    \n",
    "    def train(self,x_train,y_train,x_test,y_test,num_iterations,learning_rate=0.5):\n",
    "        \n",
    "        train_loss=[]\n",
    "        test_loss=[]\n",
    "        m_train=y_train.shape[1]\n",
    "        m_test=y_test.shape[1]\n",
    "        #one_hot_y=np.eye(3)[y_train]\n",
    "        for i in range(num_iterations):\n",
    "            \n",
    "            self.forward_propgation(x_train,y_train)\n",
    "            self.backward_propagation(x_train,y_train)\n",
    "            self.W1=self.W1-learning_rate*self.grad_cache[\"dW1\"]\n",
    "            print(self.W1)\n",
    "            self.W2=self.W2-learning_rate*self.grad_cache[\"dW2\"]\n",
    "            self.b1=self.b1-learning_rate*self.grad_cache[\"db1\"]\n",
    "            self.b2=self.b2-learning_rate*self.grad_cache[\"db2\"]\n",
    "            print\n",
    "            #this.optimized_cache[\"b1\"]=this.optimized_cache[\"b1\"]-learning_rate*this.grad_cache[\"db1\"]\n",
    "            #this.optimized_cache[\"b2\"]=this.optimized_cache[\"b2\"]-learning_rate*this.grad_cache[\"db2\"]    \n",
    "               \n",
    "            #print(self.cache[\"A2\"])\n",
    "            #print(one_hot_y)\n",
    "            if((i%10)==0):\n",
    "                \n",
    "                print(\"The train loss is\",loss(m_train,self.cache[\"A2\"],y_train))\n",
    "                train_loss.append(loss(m_train,self.cache[\"A2\"],y_train))\n",
    "                self.forward_propgation(x_test,y_test)\n",
    "                print(\"The val loss is\",loss(m_test,self.cache[\"A2\"],y_test))\n",
    "                test_loss.append(loss(m_test,self.cache[\"A2\"],y_test))\n",
    "        return train_loss,test_loss\n",
    "    #def predict():\n",
    "        \n",
    "    \n",
    "    #def accuracy():\n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The m_train is 134\n",
      "[[ 0.00649702 -0.00189452]]\n",
      "The train loss is 92.88633345233104\n",
      "The val loss is 45.74903508483727\n",
      "[[ 0.00503528 -0.0033353 ]]\n",
      "[[ 0.00408421 -0.00427272]]\n",
      "[[ 0.00352894 -0.00482001]]\n",
      "[[ 0.0032824 -0.005063 ]]\n",
      "[[ 0.00327945 -0.00506591]]\n",
      "[[ 0.0034723  -0.00487584]]\n",
      "[[ 0.00382703 -0.00452622]]\n",
      "[[ 0.00432087 -0.00403951]]\n",
      "[[ 0.0049402  -0.00342912]]\n",
      "[[ 0.00567901 -0.00270098]]\n",
      "The train loss is 92.86744969823674\n",
      "The val loss is 45.777349193834\n",
      "[[ 0.00653786 -0.00185453]]\n",
      "[[ 0.00752313 -0.0008835 ]]\n",
      "[[0.00864657 0.00022371]]\n",
      "[[0.00992511 0.00148376]]\n",
      "[[0.01138085 0.00291846]]\n",
      "[[0.01304134 0.00455493]]\n",
      "[[0.01493988 0.00642601]]\n",
      "[[0.0171162  0.00857084]]\n",
      "[[0.01961716 0.01103561]]\n",
      "[[0.02249774 0.01387449]]\n",
      "The train loss is 92.8668245438334\n",
      "The val loss is 45.78798306457124\n",
      "[[0.02582219 0.01715081]]\n",
      "[[0.02966544 0.02093842]]\n",
      "[[0.03411479 0.02532331]]\n",
      "[[0.0392718  0.03040559]]\n",
      "[[0.04525459 0.03630166]]\n",
      "[[0.05220049 0.04314684]]\n",
      "[[0.06026909 0.05109839]]\n",
      "[[0.06964573 0.06033893]]\n",
      "[[0.08054556 0.07108046]]\n",
      "[[0.09321809 0.0835688 ]]\n",
      "The train loss is 92.87013484140417\n",
      "The val loss is 45.811560012481024\n",
      "[[0.10795229 0.09808869]]\n",
      "[[0.12508232 0.11496931]]\n",
      "[[0.14499367 0.13459044]]\n",
      "[[0.16812963 0.15738874]]\n",
      "[[0.19499778 0.18386415]]\n",
      "[[0.22617591 0.21458573]]\n",
      "[[0.26231654 0.25019602]]\n",
      "[[0.30414874 0.29141283]]\n",
      "[[0.35247541 0.3390265 ]]\n",
      "[[0.40816361 0.39389033]]\n",
      "The train loss is 92.96088818411111\n",
      "The val loss is 45.956730897034255\n",
      "[[0.47212529 0.45690149]]\n",
      "[[0.54528578 0.52896997]]\n",
      "[[0.6285388  0.61097423]]\n",
      "[[0.72268944 0.70370512]]\n",
      "[[0.82839133 0.80780407]]\n",
      "[[0.9460885  0.92370611]]\n",
      "[[1.07597586 1.05160111]]\n",
      "[[1.21798936 1.19142429]]\n",
      "[[1.37182945 1.34287937]]\n",
      "[[1.53701072 1.50548729]]\n",
      "The train loss is 95.56475698711674\n",
      "The val loss is 47.67606149215789\n",
      "[[1.7129232  1.67864646]]\n",
      "[[1.89889033 1.86168971]]\n",
      "[[2.09421384 2.05392853]]\n",
      "[[2.29820301 2.25468196]]\n",
      "[[2.51019066 2.46329253]]\n",
      "[[2.72954032 2.67913343]]\n",
      "[[2.95564857 2.90161083]]\n",
      "[[3.18794524 3.13016405]]\n",
      "[[3.42589283 3.36426494]]\n",
      "[[3.6689858  3.60341706]]\n",
      "The train loss is 103.6131683524197\n",
      "The val loss is 51.705175008353926\n",
      "[[3.91674987 3.84715487]]\n",
      "[[4.1687412  4.09504281]]\n",
      "[[4.42454542 4.34667435]]\n",
      "[[4.68377659 4.60167083]]\n",
      "[[4.9460759  4.85968022]]\n",
      "[[5.21111033 5.12037574]]\n",
      "[[5.47857118 5.38345444]]\n",
      "[[5.74817258 5.64863577]]\n",
      "[[6.01965005 5.91566005]]\n",
      "[[6.29275898 6.18428715]]\n",
      "The train loss is 111.86750497060514\n",
      "The val loss is 55.71107480992123\n",
      "[[6.56727327 6.45429504]]\n",
      "[[6.84298399 6.72547852]]\n",
      "[[7.11969814 6.997648  ]]\n",
      "[[7.39723742 7.27062835]]\n",
      "[[7.67543721 7.54425781]]\n",
      "[[7.95414548 7.81838702]]\n",
      "[[8.23322189 8.09287806]]\n",
      "[[8.51253693 8.36760364]]\n",
      "[[8.79197104 8.64244628]]\n",
      "[[9.07141396 8.91729757]]\n",
      "The train loss is 120.08281603158562\n",
      "The val loss is 59.69764171647172\n",
      "[[9.35076397 9.19205754]]\n",
      "[[9.62992729 9.46663401]]\n",
      "[[9.90881752 9.74094202]]\n",
      "[[10.18735502 10.01490328]]\n",
      "[[10.46546649 10.28844574]]\n",
      "[[10.74308449 10.56150306]]\n",
      "[[11.02014699 10.83401425]]\n",
      "[[11.29659698 11.10592327]]\n",
      "[[11.57238214 11.37717867]]\n",
      "[[11.84745448 11.64773323]]\n",
      "The train loss is 128.0189307850714\n",
      "The val loss is 63.549260496656\n",
      "[[12.12177003 11.91754372]]\n",
      "[[12.39528854 12.18657056]]\n",
      "[[12.66797323 12.45477757]]\n",
      "[[12.93979056 12.72213175]]\n",
      "[[13.21070994 12.98860304]]\n",
      "[[13.48070361 13.2541641 ]]\n",
      "[[13.74974633 13.51879013]]\n",
      "[[14.0178153  13.78245868]]\n",
      "[[14.28488994 14.0451495 ]]\n",
      "[[14.55095173 14.30684438]]\n",
      "The train loss is 135.51517751205387\n",
      "The val loss is 67.18974820717942\n",
      "[[14.81598407 14.56752699]]\n",
      "[[15.07997217 14.82718276]]\n",
      "[[15.34290287 15.08579876]]\n",
      "[[15.60476459 15.34336358]]\n",
      "[[15.86554715 15.59986721]]\n",
      "[[16.12524173 15.85530096]]\n",
      "[[16.38384073 16.10965735]]\n",
      "[[16.64133771 16.36293005]]\n",
      "[[16.8977273  16.61511376]]\n",
      "[[17.15300512 16.86620416]]\n",
      "The train loss is 142.5223176853454\n",
      "The val loss is 70.59548425762576\n",
      "[[17.40716772 17.11619786]]\n",
      "[[17.66021248 17.36509228]]\n",
      "[[17.9121376  17.61288563]]\n",
      "[[18.16294202 17.85957687]]\n",
      "[[18.41262533 18.10516559]]\n",
      "[[18.66118779 18.34965204]]\n",
      "[[18.90863022 18.59303703]]\n",
      "[[19.15495401 18.83532189]]\n",
      "[[19.40016102 19.07650848]]\n",
      "[[19.6442536  19.31659909]]\n",
      "The train loss is 149.04781281196142\n",
      "The val loss is 73.76977697636973\n",
      "[[19.88723451 19.55559644]]\n",
      "[[20.12910693 19.79350363]]\n",
      "[[20.3698744  20.03032415]]\n",
      "[[20.60954077 20.26606179]]\n",
      "[[20.84811024 20.50072066]]\n",
      "[[21.08558726 20.73430516]]\n",
      "[[21.32197658 20.96681992]]\n",
      "[[21.55728315 21.19826983]]\n",
      "[[21.79151216 21.42865999]]\n",
      "[[22.02466902 21.6579957 ]]\n",
      "The train loss is 155.12307094649765\n",
      "The val loss is 76.72740693097295\n",
      "[[22.25675928 21.88628243]]\n",
      "[[22.48778869 22.11352583]]\n",
      "[[22.71776315 22.33973169]]\n",
      "[[22.94668867 22.56490594]]\n",
      "[[23.17457141 22.78905461]]\n",
      "[[23.40141764 23.01218387]]\n",
      "[[23.6272337  23.23429997]]\n",
      "[[23.85202605 23.45540924]]\n",
      "[[24.07580121 23.6755181 ]]\n",
      "[[24.29856578 23.89463304]]\n",
      "The train loss is 160.78743706512515\n",
      "The val loss is 79.48702285294823\n",
      "[[24.52032641 24.11276059]]\n",
      "[[24.7410898  24.32990734]]\n",
      "[[24.9608627  24.54607993]]\n",
      "[[25.17965188 24.76128501]]\n",
      "[[25.39746416 24.9755293 ]]\n",
      "[[25.61430637 25.1888195 ]]\n",
      "[[25.83018535 25.40116235]]\n",
      "[[26.04510798 25.61256459]]\n",
      "[[26.2590811  25.82303298]]\n",
      "[[26.4721116  26.03257426]]\n",
      "The train loss is 166.08088237228557\n",
      "The val loss is 82.06764533032386\n",
      "[[26.68420634 26.2411952 ]]\n",
      "[[26.89537216 26.44890252]]\n",
      "[[27.10561593 26.65570296]]\n",
      "[[27.31494446 26.86160324]]\n",
      "[[27.52336458 27.06661006]]\n",
      "[[27.73088308 27.2707301 ]]\n",
      "[[27.93750672 27.47397002]]\n",
      "[[28.14324226 27.67633644]]\n",
      "[[28.34809639 27.87783597]]\n",
      "[[28.55207582 28.07847519]]\n",
      "The train loss is 171.0409036897168\n",
      "The val loss is 84.48716919297448\n",
      "[[28.75518719 28.27826063]]\n",
      "[[28.95743711 28.47719879]]\n",
      "[[29.15883217 28.67529616]]\n",
      "[[29.35937891 28.87255916]]\n",
      "[[29.55908383 29.06899419]]\n",
      "[[29.7579534 29.2646076]]\n",
      "[[29.95599403 29.45940571]]\n",
      "[[30.15321209 29.65339479]]\n",
      "[[30.34961394 29.84658106]]\n",
      "[[30.54520584 30.03897071]]\n",
      "The train loss is 175.70140897090573\n",
      "The val loss is 86.76181439833456\n",
      "[[30.73999405 30.23056988]]\n",
      "[[30.93398476 30.42138466]]\n",
      "[[31.12718412 30.6114211 ]]\n",
      "[[31.31959823 30.8006852 ]]\n",
      "[[31.51123315 30.98918292]]\n",
      "[[31.70209488 31.17692015]]\n",
      "[[31.89218938 31.36390276]]\n",
      "[[32.08152255 31.55013656]]\n",
      "[[32.27010025 31.73562731]]\n",
      "[[32.4579283  31.92038071]]\n",
      "The train loss is 180.09251594622128\n",
      "The val loss is 88.90601629789121\n",
      "[[32.64501245 32.10440245]]\n",
      "[[32.83135841 32.28769812]]\n",
      "[[33.01697185 32.4702733 ]]\n",
      "[[33.20185836 32.65213349]]\n",
      "[[33.38602351 32.83328418]]\n",
      "[[33.56947281 33.01373078]]\n",
      "[[33.75221172 33.19347866]]\n",
      "[[33.93424565 33.37253314]]\n",
      "[[34.11557996 33.55089949]]\n",
      "[[34.29621997 33.72858293]]\n",
      "The train loss is 184.2407490640627\n",
      "The val loss is 90.93250970037388\n",
      "[[34.47617093 33.90558865]]\n",
      "[[34.65543807 34.08192178]]\n",
      "[[34.83402654 34.25758738]]\n",
      "[[35.01194147 34.43259049]]\n",
      "[[35.18918791 34.60693611]]\n",
      "[[35.3657709  34.78062915]]\n",
      "[[35.54169541 34.95367453]]\n",
      "[[35.71696635 35.12607707]]\n",
      "[[35.89158861 35.29784159]]\n",
      "[[36.06556703 35.46897282]]\n",
      "The train loss is 188.16938902881154\n",
      "The val loss is 92.85248878783723\n",
      "[[36.23890638 35.63947548]]\n",
      "[[36.4116114  35.80935423]]\n",
      "[[36.58368679 35.97861368]]\n",
      "[[36.7551372  36.14725841]]\n",
      "[[36.92596723 36.31529294]]\n",
      "[[37.09618144 36.48272175]]\n",
      "[[37.26578434 36.64954929]]\n",
      "[[37.43478041 36.81577996]]\n",
      "[[37.60317406 36.9814181 ]]\n",
      "[[37.77096969 37.14646802]]\n",
      "The train loss is 191.89885967308467\n",
      "The val loss is 94.67578710315416\n",
      "[[37.93817163 37.310934  ]]\n",
      "[[38.10478419 37.47482027]]\n",
      "[[38.27081161 37.63813101]]\n",
      "[[38.43625812 37.80087036]]\n",
      "[[38.60112789 37.96304244]]\n",
      "[[38.76542505 38.1246513 ]]\n",
      "[[38.9291537  38.28570097]]\n",
      "[[39.09231788 38.44619545]]\n",
      "[[39.25492161 38.60613866]]\n",
      "[[39.41696886 38.76553453]]\n",
      "The train loss is 195.44710042465434\n",
      "The val loss is 96.41105226776337\n",
      "[[39.57846358 38.92438693]]\n",
      "[[39.73940965 39.08269967]]\n",
      "[[39.89981094 39.24047658]]\n",
      "[[40.05967126 39.39772139]]\n",
      "[[40.2189944  39.55443783]]\n",
      "[[40.37778411 39.71062958]]\n",
      "[[40.5360441  39.86630031]]\n",
      "[[40.69377804 40.02145361]]\n",
      "[[40.85098957 40.17609308]]\n",
      "[[41.0076823  40.33022224]]\n",
      "The train loss is 198.8299035102253\n",
      "The val loss is 98.06590498504934\n",
      "[[41.16385979 40.48384462]]\n",
      "[[41.31952557 40.63696369]]\n",
      "[[41.47468316 40.78958289]]\n",
      "[[41.62933601 40.94170564]]\n",
      "[[41.78348756 41.0933353 ]]\n",
      "[[41.9371412  41.24447522]]\n",
      "[[42.0903003  41.39512871]]\n",
      "[[42.24296821 41.54529906]]\n",
      "[[42.39514821 41.69498951]]\n",
      "[[42.54684359 41.84420327]]\n",
      "The train loss is 202.06120983100303\n",
      "The val loss is 99.64707908170901\n",
      "[[42.69805758 41.99294354]]\n",
      "[[42.84879339 42.14121347]]\n",
      "[[42.9990542  42.28901619]]\n",
      "[[43.14884316 42.43635479]]\n",
      "[[43.29816338 42.58323235]]\n",
      "[[43.44701795 42.72965189]]\n",
      "[[43.59540994 42.87561642]]\n",
      "[[43.74334238 43.02112894]]\n",
      "[[43.89081825 43.16619238]]\n",
      "[[44.03784055 43.31080968]]\n",
      "The train loss is 205.15336425901774\n",
      "The val loss is 101.16054269657057\n",
      "[[44.18441221 43.45498373]]\n",
      "[[44.33053616 43.5987174 ]]\n",
      "[[44.47621528 43.74201354]]\n",
      "[[44.62145243 43.88487495]]\n",
      "[[44.76625046 44.02730444]]\n",
      "[[44.91061218 44.16930477]]\n",
      "[[45.05454036 44.31087867]]\n",
      "[[45.19803777 44.45202886]]\n",
      "[[45.34110714 44.59275803]]\n",
      "[[45.48375118 44.73306883]]\n",
      "The train loss is 208.11733396675328\n",
      "The val loss is 102.61160216923753\n",
      "[[45.62597258 44.87296392]]\n",
      "[[45.76777398 45.01244589]]\n",
      "[[45.90915802 45.15151735]]\n",
      "[[46.05012732 45.29018086]]\n",
      "[[46.19068446 45.42843897]]\n",
      "[[46.330832   45.56629418]]\n",
      "[[46.47057247 45.70374901]]\n",
      "[[46.60990841 45.84080593]]\n",
      "[[46.74884229 45.97746738]]\n",
      "[[46.8873766 46.1137358]]\n",
      "The train loss is 210.96289433741356\n",
      "The val loss is 104.00499068087542\n",
      "[[47.02551377 46.24961359]]\n",
      "[[47.16325623 46.38510315]]\n",
      "[[47.3006064  46.52020683]]\n",
      "[[47.43756665 46.65492699]]\n",
      "[[47.57413933 46.78926594]]\n",
      "[[47.71032681 46.92322598]]\n",
      "[[47.84613138 47.0568094 ]]\n",
      "[[47.98155536 47.19001846]]\n",
      "[[48.11660102 47.3228554 ]]\n",
      "[[48.25127062 47.45532244]]\n",
      "The train loss is 213.69878702905888\n",
      "The val loss is 105.34494374924998\n",
      "[[48.3855664  47.58742178]]\n",
      "[[48.51949058 47.71915561]]\n",
      "[[48.65304535 47.85052609]]\n",
      "[[48.7862329  47.98153536]]\n",
      "[[48.91905539 48.11218555]]\n",
      "[[49.05151496 48.24247877]]\n",
      "[[49.18361373 48.3724171 ]]\n",
      "[[49.31535381 48.50200262]]\n",
      "[[49.44673729 48.63123738]]\n",
      "[[49.57776624 48.76012341]]\n",
      "The train loss is 216.3328543962965\n",
      "The val loss is 106.63526352959265\n",
      "[[49.7084427  48.88866273]]\n",
      "[[49.83876871 49.01685734]]\n",
      "[[49.96874629 49.14470923]]\n",
      "[[50.09837744 49.27222036]]\n",
      "[[50.22766413 49.39939268]]\n",
      "[[50.35660833 49.52622812]]\n",
      "[[50.485212   49.65272859]]\n",
      "[[50.61347705 49.77889601]]\n",
      "[[50.74140541 49.90473225]]\n",
      "[[50.86899898 50.03023918]]\n",
      "The train loss is 218.87215396320144\n",
      "The val loss is 107.87937364722553\n",
      "[[50.99625964 50.15541865]]\n",
      "[[51.12318926 50.28027251]]\n",
      "[[51.24978969 50.40480256]]\n",
      "[[51.37606276 50.52901062]]\n",
      "[[51.5020103  50.65289848]]\n",
      "[[51.62763411 50.77646791]]\n",
      "[[51.75293599 50.89972068]]\n",
      "[[51.8779177  51.02265854]]\n",
      "[[52.00258102 51.14528321]]\n",
      "[[52.12692768 51.26759641]]\n",
      "The train loss is 221.32305611324412\n",
      "The val loss is 109.08036604812239\n",
      "[[52.25095942 51.38959985]]\n",
      "[[52.37467797 51.51129522]]\n",
      "[[52.49808501 51.6326842 ]]\n",
      "[[52.62118225 51.75376844]]\n",
      "[[52.74397136 51.8745496 ]]\n",
      "[[52.866454   51.99502931]]\n",
      "[[52.98863183 52.1152092 ]]\n",
      "[[53.11050648 52.23509087]]\n",
      "[[53.23207956 52.35467592]]\n",
      "[[53.3533527  52.47396593]]\n",
      "The train loss is 223.69132767115366\n",
      "The val loss is 110.24104112766464\n",
      "[[53.47432749 52.59296248]]\n",
      "[[53.59500552 52.71166713]]\n",
      "[[53.71538835 52.83008141]]\n",
      "[[53.83547754 52.94820687]]\n",
      "[[53.95527465 53.06604502]]\n",
      "[[54.0747812  53.18359738]]\n",
      "[[54.19399872 53.30086544]]\n",
      "[[54.31292872 53.41785068]]\n",
      "[[54.4315727  53.53455459]]\n",
      "[[54.54993214 53.65097862]]\n",
      "The train loss is 225.98220361865026\n",
      "The val loss is 111.36394219691495\n",
      "[[54.66800852 53.76712422]]\n",
      "[[54.7858033  53.88299284]]\n",
      "[[54.90331794 53.9985859 ]]\n",
      "[[55.02055387 54.11390482]]\n",
      "[[55.13751253 54.228951  ]]\n",
      "[[55.25419534 54.34372585]]\n",
      "[[55.3706037  54.45823074]]\n",
      "[[55.486739   54.57246705]]\n",
      "[[55.60260264 54.68643615]]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[55.718196   54.80013938]]\n",
      "The train loss is 228.2004488149878\n",
      "The val loss is 112.45138517256859\n",
      "[[55.83352042 54.91357809]]\n",
      "[[55.94857728 55.02675361]]\n",
      "[[56.06336792 55.13966727]]\n",
      "[[56.17789366 55.25232037]]\n",
      "[[56.29215583 55.36471422]]\n",
      "[[56.40615575 55.47685011]]\n",
      "[[56.51989473 55.58872932]]\n",
      "[[56.63337404 55.70035313]]\n",
      "[[56.74659499 55.8117228 ]]\n",
      "[[56.85955884 55.92283958]]\n",
      "The train loss is 230.3504112807133\n",
      "The val loss is 113.50548423023163\n",
      "[[56.97226686 56.03370472]]\n",
      "[[57.08472031 56.14431945]]\n",
      "[[57.19692043 56.25468501]]\n",
      "[[57.30886846 56.3648026 ]]\n",
      "[[57.42056563 56.47467343]]\n",
      "[[57.53201315 56.58429871]]\n",
      "[[57.64321224 56.69367963]]\n",
      "[[57.75416411 56.80281736]]\n",
      "[[57.86486993 56.91171308]]\n",
      "[[57.9753309  57.02036795]]\n",
      "The train loss is 232.43606834238182\n",
      "The val loss is 114.52817403806054\n",
      "[[58.08554819 57.12878314]]\n",
      "[[58.19552296 57.23695978]]\n",
      "[[58.30525638 57.34489901]]\n",
      "[[58.4147496  57.45260198]]\n",
      "[[58.52400375 57.56006979]]\n",
      "[[58.63301998 57.66730357]]\n",
      "[[58.7417994  57.77430442]]\n",
      "[[58.85034312 57.88107344]]\n",
      "[[58.95865227 57.98761172]]\n",
      "[[59.06672795 58.09392035]]\n",
      "The train loss is 234.4610667197738\n",
      "The val loss is 115.52122908581977\n",
      "[[59.17457123 58.20000039]]\n",
      "[[59.28218322 58.30585292]]\n",
      "[[59.38956499 58.411479  ]]\n",
      "[[59.4967176  58.51687967]]\n",
      "[[59.60364212 58.62205599]]\n",
      "[[59.71033961 58.72700899]]\n",
      "[[59.81681112 58.83173971]]\n",
      "[[59.92305768 58.93624916]]\n",
      "[[60.02908032 59.04053836]]\n",
      "[[60.13488008 59.14460833]]\n",
      "The train loss is 236.4287574584092\n",
      "The val loss is 116.48628053989377\n",
      "[[60.24045797 59.24846005]]\n",
      "[[60.345815   59.35209454]]\n",
      "[[60.45095218 59.45551277]]\n",
      "[[60.5558705  59.55871573]]\n",
      "[[60.66057096 59.66170439]]\n",
      "[[60.76505454 59.76447971]]\n",
      "[[60.86932222 59.86704267]]\n",
      "[[60.97337497 59.96939422]]\n",
      "[[61.07721374 60.0715353 ]]\n",
      "[[61.1808395  60.17346685]]\n",
      "The train loss is 238.34222646248878\n",
      "The val loss is 117.42483098482707\n",
      "[[61.28425321 60.27518981]]\n",
      "[[61.38745579 60.37670511]]\n",
      "[[61.4904482  60.47801367]]\n",
      "[[61.59323136 60.57911641]]\n",
      "[[61.69580619 60.68001424]]\n",
      "[[61.79817363 60.78070806]]\n",
      "[[61.90033457 60.88119876]]\n",
      "[[62.00228993 60.98148725]]\n",
      "[[62.1040406 61.0815744]]\n",
      "[[62.20558749 61.1814611 ]]\n",
      "The train loss is 240.20432126141424\n",
      "The val loss is 118.33826735407479\n",
      "[[62.30693147 61.28114822]]\n",
      "[[62.40807344 61.38063663]]\n",
      "[[62.50901427 61.47992719]]\n",
      "[[62.60975483 61.57902076]]\n",
      "[[62.71029598 61.67791819]]\n",
      "[[62.81063859 61.77662032]]\n",
      "[[62.91078351 61.87512799]]\n",
      "[[63.01073158 61.97344205]]\n",
      "[[63.11048366 62.07156331]]\n",
      "[[63.21004057 62.1694926 ]]\n",
      "The train loss is 242.017674542159\n",
      "The val loss is 119.22787230468407\n",
      "[[63.30940315 62.26723075]]\n",
      "[[63.40857223 62.36477855]]\n",
      "[[63.50754863 62.46213683]]\n",
      "[[63.60633315 62.55930637]]\n",
      "[[63.70492662 62.65628799]]\n",
      "[[63.80332984 62.75308247]]\n",
      "[[63.90154361 62.8496906 ]]\n",
      "[[63.99956871 62.94611315]]\n",
      "[[64.09740595 63.04235092]]\n",
      "[[64.19505611 63.13840466]]\n",
      "The train loss is 243.78472489620225\n",
      "The val loss is 120.09483425085322\n",
      "[[64.29251997 63.23427515]]\n",
      "[[64.3897983  63.32996315]]\n",
      "[[64.48689187 63.42546941]]\n",
      "[[64.58380144 63.52079469]]\n",
      "[[64.68052778 63.61593973]]\n",
      "[[64.77707165 63.71090528]]\n",
      "[[64.87343379 63.80569208]]\n",
      "[[64.96961494 63.90030086]]\n",
      "[[65.06561586 63.99473235]]\n",
      "[[65.16143727 64.08898727]]\n",
      "The train loss is 245.50773516038203\n",
      "The val loss is 120.94025623824953\n",
      "[[65.25707991 64.18306635]]\n",
      "[[65.35254451 64.27697029]]\n",
      "[[65.44783178 64.37069982]]\n",
      "[[65.54294245 64.46425563]]\n",
      "[[65.63787723 64.55763843]]\n",
      "[[65.73263683 64.65084891]]\n",
      "[[65.82722196 64.74388778]]\n",
      "[[65.92163331 64.83675572]]\n",
      "[[66.01587159 64.92945341]]\n",
      "[[66.10993748 65.02198153]]\n",
      "The train loss is 247.18880867331686\n",
      "The val loss is 121.76516381343514\n",
      "[[66.20383167 65.11434077]]\n",
      "[[66.29755484 65.20653179]]\n",
      "[[66.39110769 65.29855526]]\n",
      "[[66.48449087 65.39041185]]\n",
      "[[66.57770507 65.48210222]]\n",
      "[[66.67075095 65.57362702]]\n",
      "[[66.76362917 65.66498691]]\n",
      "[[66.85634039 65.75618254]]\n",
      "[[66.94888527 65.84721454]]\n",
      "[[67.04126445 65.93808356]]\n",
      "The train loss is 248.82990372091888\n",
      "The val loss is 122.57051201975163\n",
      "[[67.1334786  66.02879024]]\n",
      "[[67.22552834 66.11933521]]\n",
      "[[67.31741433 66.2097191 ]]\n",
      "[[67.40913718 66.29994253]]\n",
      "[[67.50069755 66.39000613]]\n",
      "[[67.59209605 66.47991051]]\n",
      "[[67.68333332 66.56965629]]\n",
      "[[67.77440996 66.65924408]]\n",
      "[[67.8653266  66.74867449]]\n",
      "[[67.95608386 66.83794812]]\n",
      "The train loss is 250.43284640426307\n",
      "The val loss is 123.35719163176631\n",
      "[[68.04668233 66.92706556]]\n",
      "[[68.13712264 67.01602743]]\n",
      "[[68.22740538 67.1048343 ]]\n",
      "[[68.31753115 67.19348677]]\n",
      "[[68.40750054 67.28198543]]\n",
      "[[68.49731416 67.37033086]]\n",
      "[[68.58697258 67.45852363]]\n",
      "[[68.6764764  67.54656433]]\n",
      "[[68.76582619 67.63445352]]\n",
      "[[68.85502254 67.72219178]]\n",
      "The train loss is 251.9993421293237\n",
      "The val loss is 124.1260347242273\n",
      "[[68.94406602 67.80977967]]\n",
      "[[69.0329572  67.89721776]]\n",
      "[[69.12169665 67.98450659]]\n",
      "[[69.21028494 68.07164674]]\n",
      "[[69.29872263 68.15863875]]\n",
      "[[69.38701028 68.24548318]]\n",
      "[[69.47514844 68.33218056]]\n",
      "[[69.56313767 68.41873145]]\n",
      "[[69.65097852 68.50513639]]\n",
      "[[69.73867153 68.5913959 ]]\n",
      "The train loss is 253.5309858896954\n",
      "The val loss is 124.87781965787097\n",
      "[[69.82621725 68.67751054]]\n",
      "[[69.91361621 68.76348082]]\n",
      "[[70.00086896 68.84930728]]\n",
      "[[70.08797603 68.93499044]]\n",
      "[[70.17493795 69.02053083]]\n",
      "[[70.26175525 69.10592896]]\n",
      "[[70.34842846 69.19118535]]\n",
      "[[70.43495809 69.27630052]]\n",
      "[[70.52134467 69.36127498]]\n",
      "[[70.60758871 69.44610923]]\n",
      "The train loss is 255.02927148947649\n",
      "The val loss is 125.61327555294925\n",
      "[[70.69369073 69.53080379]]\n",
      "[[70.77965124 69.61535915]]\n",
      "[[70.86547074 69.69977581]]\n",
      "[[70.95114975 69.78405427]]\n",
      "[[71.03668876 69.86819502]]\n",
      "[[71.12208827 69.95219856]]\n",
      "[[71.20734879 70.03606538]]\n",
      "[[71.2924708  70.11979596]]\n",
      "[[71.3774548  70.20339079]]\n",
      "[[71.46230128 70.28685034]]\n",
      "The train loss is 256.49559983324593\n",
      "The val loss is 126.3330863116286\n",
      "[[71.54701072 70.37017509]]\n",
      "[[71.63158361 70.45336553]]\n",
      "[[71.71602042 70.53642212]]\n",
      "[[71.80032164 70.61934533]]\n",
      "[[71.88448775 70.70213564]]\n",
      "[[71.9685192 70.7847935]]\n",
      "[[72.05241649 70.86731939]]\n",
      "[[72.13618007 70.94971376]]\n",
      "[[72.2198104  71.03197706]]\n",
      "[[72.30330797 71.11410977]]\n",
      "The train loss is 257.9312863928851\n",
      "The val loss is 127.03789424216892\n",
      "[[72.38667321 71.19611232]]\n",
      "[[72.4699066  71.27798517]]\n",
      "[[72.55300859 71.35972877]]\n",
      "[[72.63597963 71.44134356]]\n",
      "[[72.71882017 71.52282999]]\n",
      "[[72.80153066 71.6041885 ]]\n",
      "[[72.88411155 71.68541952]]\n",
      "[[72.96656329 71.76652351]]\n",
      "[[73.0488863  71.84750088]]\n",
      "[[73.13108104 71.92835207]]\n",
      "The train loss is 259.33756794640595\n",
      "The val loss is 127.72830333077206\n",
      "[[73.21314794 72.00907751]]\n",
      "[[73.29508743 72.08967763]]\n",
      "[[73.37689995 72.17015285]]\n",
      "[[73.45858593 72.25050361]]\n",
      "[[73.54014579 72.3307303 ]]\n",
      "[[73.62157996 72.41083337]]\n",
      "[[73.70288887 72.49081322]]\n",
      "[[73.78407293 72.57067026]]\n",
      "[[73.86513256 72.65040492]]\n",
      "[[73.94606819 72.7300176 ]]\n",
      "The train loss is 260.7156086714832\n",
      "The val loss is 128.4048822010096\n",
      "[[74.02688022 72.8095087 ]]\n",
      "[[74.10756907 72.88887864]]\n",
      "[[74.18813515 72.96812781]]\n",
      "[[74.26857887 73.04725662]]\n",
      "[[74.34890064 73.12626548]]\n",
      "[[74.42910085 73.20515477]]\n",
      "[[74.50917991 73.28392489]]\n",
      "[[74.58913822 73.36257624]]\n",
      "[[74.66897618 73.4411092 ]]\n",
      "[[74.74869419 73.51952417]]\n",
      "The train loss is 262.0665056657507\n",
      "The val loss is 129.06816679561362\n",
      "[[74.82829264 73.59782154]]\n",
      "[[74.90777191 73.67600169]]\n",
      "[[74.98713241 73.754065  ]]\n",
      "[[75.06637452 73.83201186]]\n",
      "[[75.14549863 73.90984265]]\n",
      "[[75.22450511 73.98755773]]\n",
      "[[75.30339436 74.0651575 ]]\n",
      "[[75.38216675 74.14264233]]\n",
      "[[75.46082266 74.22001258]]\n",
      "[[75.53936247 74.29726862]]\n",
      "The train loss is 263.39129395679856\n",
      "The val loss is 129.71866281102385\n",
      "[[75.61778655 74.37441084]]\n",
      "[[75.69609528 74.45143958]]\n",
      "[[75.77428902 74.52835522]]\n",
      "[[75.85236816 74.60515813]]\n",
      "[[75.93033304 74.68184865]]\n",
      "[[76.00818405 74.75842716]]\n",
      "[[76.08592153 74.83489401]]\n",
      "[[76.16354587 74.91124955]]\n",
      "[[76.2410574  74.98749415]]\n",
      "[[76.31845651 75.06362815]]\n",
      "The train loss is 264.6909510569545\n",
      "The val loss is 130.3568479113121\n",
      "[[76.39574353 75.1396519 ]]\n",
      "[[76.47291883 75.21556575]]\n",
      "[[76.54998275 75.29137005]]\n",
      "[[76.62693566 75.36706515]]\n",
      "[[76.7037779  75.44265139]]\n",
      "[[76.78050981 75.51812912]]\n",
      "[[76.85713175 75.59349866]]\n",
      "[[76.93364406 75.66876037]]\n",
      "[[77.01004708 75.74391458]]\n",
      "[[77.08634116 75.81896163]]\n",
      "The train loss is 265.9664011111951\n",
      "The val loss is 130.98317374484645\n",
      "[[77.16252662 75.89390184]]\n",
      "[[77.23860383 75.96873556]]\n",
      "[[77.31457309 76.04346311]]\n",
      "[[77.39043477 76.11808482]]\n",
      "[[77.46618918 76.19260103]]\n",
      "[[77.54183666 76.26701205]]\n",
      "[[77.61737753 76.34131822]]\n",
      "[[77.69281214 76.41551985]]\n",
      "[[77.7681408  76.48961726]]\n",
      "[[77.84336383 76.56361079]]\n",
      "The train loss is 267.2185186806866\n",
      "The val loss is 131.5980677842462\n",
      "[[77.91848158 76.63750074]]\n",
      "[[77.99349434 76.71128743]]\n",
      "[[78.06840246 76.78497119]]\n",
      "[[78.14320624 76.85855231]]\n",
      "[[78.21790601 76.93203112]]\n",
      "[[78.29250207 77.00540793]]\n",
      "[[78.36699476 77.07868305]]\n",
      "[[78.44138437 77.15185677]]\n",
      "[[78.51567122 77.22492943]]\n",
      "[[78.58985562 77.2979013 ]]\n",
      "The train loss is 268.4481321994043\n",
      "The val loss is 132.201935007747\n",
      "[[78.66393789 77.37077271]]\n",
      "[[78.73791832 77.44354396]]\n",
      "[[78.81179723 77.51621533]]\n",
      "[[78.88557492 77.58878715]]\n",
      "[[78.95925169 77.66125969]]\n",
      "[[79.03282784 77.73363327]]\n",
      "[[79.10630368 77.80590817]]\n",
      "[[79.17967951 77.87808469]]\n",
      "[[79.25295561 77.95016313]]\n",
      "[[79.3261323  78.02214377]]\n",
      "The train loss is 269.6560271369102\n",
      "The val loss is 132.7951594379773\n",
      "[[79.39920986 78.09402691]]\n",
      "[[79.47218859 78.16581283]]\n",
      "[[79.54506877 78.23750182]]\n",
      "[[79.61785071 78.30909417]]\n",
      "[[79.69053469 78.38059016]]\n",
      "[[79.763121   78.45199008]]\n",
      "[[79.83560993 78.52329421]]\n",
      "[[79.90800176 78.59450283]]\n",
      "[[79.98029677 78.66561622]]\n",
      "[[80.05249526 78.73663465]]\n",
      "The train loss is 270.8429488965466\n",
      "The val loss is 133.37810555231096\n",
      "[[80.12459749 78.80755842]]\n",
      "[[80.19660376 78.87838778]]\n",
      "[[80.26851434 78.94912303]]\n",
      "[[80.34032952 79.01976442]]\n",
      "[[80.41204956 79.09031224]]\n",
      "[[80.48367474 79.16076675]]\n",
      "[[80.55520534 79.23112823]]\n",
      "[[80.62664163 79.30139694]]\n",
      "[[80.69798389 79.37157315]]\n",
      "[[80.76923238 79.44165713]]\n",
      "The train loss is 272.0096054749864\n",
      "The val loss is 133.95111957735875\n",
      "[[80.84038737 79.51164914]]\n",
      "[[80.91144914 79.58154945]]\n",
      "[[80.98241794 79.65135831]]\n",
      "[[81.05329405 79.721076  ]]\n",
      "[[81.12407773 79.79070277]]\n",
      "[[81.19476924 79.86023888]]\n",
      "[[81.26536885 79.92968459]]\n",
      "[[81.33587681 79.99904016]]\n",
      "[[81.4062934  80.06830584]]\n",
      "[[81.47661886 80.13748188]]\n",
      "The train loss is 273.15666990618746\n",
      "The val loss is 134.5145306787586\n",
      "[[81.54685345 80.20656855]]\n",
      "[[81.61699744 80.27556609]]\n",
      "[[81.68705108 80.34447475]]\n",
      "[[81.75701462 80.4132948 ]]\n",
      "[[81.82688831 80.48202646]]\n",
      "[[81.89667241 80.55067   ]]\n",
      "[[81.96636718 80.61922566]]\n",
      "[[82.03597285 80.68769369]]\n",
      "[[82.10548968 80.75607433]]\n",
      "[[82.17491792 80.82436783]]\n",
      "The train loss is 274.2847825102525\n",
      "The val loss is 135.0686520561989\n",
      "[[82.24425781 80.89257442]]\n",
      "[[82.3135096  80.96069436]]\n",
      "[[82.38267354 81.02872787]]\n",
      "[[82.45174987 81.09667521]]\n",
      "[[82.52073882 81.16453661]]\n",
      "[[82.58964066 81.23231231]]\n",
      "[[82.6584556  81.30000254]]\n",
      "[[82.7271839  81.36760755]]\n",
      "[[82.79582579 81.43512755]]\n",
      "[[82.86438152 81.5025628 ]]\n",
      "The train loss is 275.3945529654721\n",
      "The val loss is 135.61378195253522\n",
      "[[82.93285131 81.56991352]]\n",
      "[[83.0012354  81.63717995]]\n",
      "[[83.06953403 81.70436231]]\n",
      "[[83.13774743 81.77146083]]\n",
      "[[83.20587583 81.83847575]]\n",
      "[[83.27391947 81.90540729]]\n",
      "[[83.34187858 81.97225568]]\n",
      "[[83.40975338 82.03902114]]\n",
      "[[83.47754411 82.1057039 ]]\n",
      "[[83.54525099 82.17230419]]\n",
      "The train loss is 276.4865622198814\n",
      "The val loss is 136.1502045849125\n",
      "[[83.61287425 82.23882222]]\n",
      "[[83.68041411 82.30525823]]\n",
      "[[83.7478708  82.37161242]]\n",
      "[[83.81524455 82.43788502]]\n",
      "[[83.88253558 82.50407626]]\n",
      "[[83.94974411 82.57018634]]\n",
      "[[84.01687036 82.63621549]]\n",
      "[[84.08391455 82.70216392]]\n",
      "[[84.1508769  82.76803186]]\n",
      "[[84.21775763 82.8338195 ]]\n",
      "The train loss is 277.56136425692625\n",
      "The val loss is 136.67819100497687\n",
      "[[84.28455696 82.89952708]]\n",
      "[[84.3512751 82.9651548]]\n",
      "[[84.41791227 83.03070287]]\n",
      "[[84.48446869 83.09617151]]\n",
      "[[84.55094456 83.16156092]]\n",
      "[[84.61734011 83.22687132]]\n",
      "[[84.68365554 83.29210291]]\n",
      "[[84.74989107 83.35725591]]\n",
      "[[84.81604691 83.42233052]]\n",
      "[[84.88212326 83.48732694]]\n",
      "The train loss is 278.6194877283296\n",
      "The val loss is 137.19799989451846\n",
      "[[84.94812034 83.55224538]]\n",
      "[[85.01403835 83.61708606]]\n",
      "[[85.07987751 83.68184916]]\n",
      "[[85.14563801 83.74653489]]\n",
      "[[85.21132006 83.81114346]]\n",
      "[[85.27692387 83.87567507]]\n",
      "[[85.34244964 83.94012992]]\n",
      "[[85.40789758 84.0045082 ]]\n",
      "[[85.47326788 84.06881012]]\n",
      "[[85.53856075 84.13303587]]\n",
      "The train loss is 279.66143746589563\n",
      "The val loss is 137.70987830224917\n",
      "[[85.6037764  84.19718566]]\n",
      "[[85.66891501 84.26125967]]\n",
      "[[85.73397678 84.32525811]]\n",
      "[[85.79896193 84.38918117]]\n",
      "[[85.86387064 84.45302904]]\n",
      "[[85.92870311 84.51680192]]\n",
      "[[85.99345953 84.5805    ]]\n",
      "[[86.05814011 84.64412347]]\n",
      "[[86.12274503 84.70767253]]\n",
      "[[86.18727449 84.77114735]]\n",
      "The train loss is 280.68769588281305\n",
      "The val loss is 138.21406232683114\n",
      "[[86.25172868 84.83454814]]\n",
      "[[86.3161078  84.89787508]]\n",
      "[[86.38041204 84.96112837]]\n",
      "[[86.44464158 85.02430817]]\n",
      "[[86.50879661 85.0874147 ]]\n",
      "[[86.57287733 85.15044812]]\n",
      "[[86.63688393 85.21340863]]\n",
      "[[86.70081658 85.27629641]]\n",
      "[[86.76467549 85.33911165]]\n",
      "[[86.82846082 85.40185452]]\n",
      "The train loss is 281.6987242739557\n",
      "The val loss is 138.71077775077416\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[86.89217278 85.46452521]]\n",
      "[[86.95581154 85.5271239 ]]\n",
      "[[87.01937729 85.58965077]]\n",
      "[[87.08287021 85.652106  ]]\n",
      "[[87.14629049 85.71448978]]\n",
      "[[87.2096383  85.77680228]]\n",
      "[[87.27291382 85.83904367]]\n",
      "[[87.33611725 85.90121415]]\n",
      "[[87.39924875 85.96331387]]\n",
      "[[87.46230851 86.02534303]]\n",
      "The train loss is 282.6949640237505\n",
      "The val loss is 139.20024062935744\n",
      "[[87.5252967  86.08730178]]\n",
      "[[87.5882135  86.14919032]]\n",
      "[[87.6510591  86.21100882]]\n",
      "[[87.71383366 86.27275743]]\n",
      "[[87.77653735 86.33443635]]\n",
      "[[87.83917037 86.39604574]]\n",
      "[[87.90173288 86.45758578]]\n",
      "[[87.96422505 86.51905663]]\n",
      "[[88.02664705 86.58045846]]\n",
      "[[88.08899907 86.64179144]]\n",
      "The train loss is 283.6768377293504\n",
      "The val loss is 139.68265783833505\n",
      "[[88.15128127 86.70305575]]\n",
      "[[88.21349382 86.76425155]]\n",
      "[[88.27563689 86.82537901]]\n",
      "[[88.33771065 86.88643829]]\n",
      "[[88.39971528 86.94742957]]\n",
      "[[88.46165093 87.008353  ]]\n",
      "[[88.52351779 87.06920876]]\n",
      "[[88.585316 87.129997]]\n",
      "[[88.64704575 87.19071789]]\n",
      "[[88.7087072 87.2513716]]\n",
      "The train loss is 284.6447502461008\n",
      "The val loss is 140.158227583821\n",
      "[[88.77030051 87.31195828]]\n",
      "[[88.83182584 87.37247811]]\n",
      "[[88.89328337 87.43293123]]\n",
      "[[88.95467325 87.49331781]]\n",
      "[[89.01599565 87.55363802]]\n",
      "[[89.07725073 87.613892  ]]\n",
      "[[89.13843865 87.67407993]]\n",
      "[[89.19955958 87.73420195]]\n",
      "[[89.26061366 87.79425823]]\n",
      "[[89.32160107 87.85424892]]\n",
      "The train loss is 285.59908966163823\n",
      "The val loss is 140.62713987743024\n",
      "[[89.38252197 87.91417418]]\n",
      "[[89.4433765  87.97403417]]\n",
      "[[89.50416483 88.03382904]]\n",
      "[[89.56488712 88.09355895]]\n",
      "[[89.62554353 88.15322404]]\n",
      "[[89.6861342  88.21282449]]\n",
      "[[89.7466593  88.27236043]]\n",
      "[[89.80711898 88.33183202]]\n",
      "[[89.86751339 88.39123941]]\n",
      "[[89.9278427  88.45058276]]\n",
      "The train loss is 286.5402282043602\n",
      "The val loss is 141.0895769794656\n",
      "[[89.98810705 88.50986221]]\n",
      "[[90.04830659 88.56907792]]\n",
      "[[90.10844149 88.62823004]]\n",
      "[[90.16851188 88.68731871]]\n",
      "[[90.22851793 88.74634408]]\n",
      "[[90.28845977 88.80530631]]\n",
      "[[90.34833758 88.86420553]]\n",
      "[[90.40815148 88.92304191]]\n",
      "[[90.46790164 88.98181557]]\n",
      "[[90.52758819 89.04052668]]\n",
      "The train loss is 287.46852309147727\n",
      "The val loss is 141.54571381268408\n",
      "[[90.5872113  89.09917538]]\n",
      "[[90.6467711 89.1577618]]\n",
      "[[90.70626774 89.2162861 ]]\n",
      "[[90.76570138 89.27474842]]\n",
      "[[90.82507214 89.33314891]]\n",
      "[[90.88438019 89.3914877 ]]\n",
      "[[90.94362567 89.44976493]]\n",
      "[[91.00280871 89.50798076]]\n",
      "[[91.06192947 89.56613533]]\n",
      "[[91.12098809 89.62422876]]\n",
      "The train loss is 288.38431732138645\n",
      "The val loss is 141.99571834894613\n",
      "[[91.17998471 89.68226121]]\n",
      "[[91.23891947 89.74023281]]\n",
      "[[91.29779251 89.79814371]]\n",
      "[[91.35660398 89.85599404]]\n",
      "[[91.41535402 89.91378394]]\n",
      "[[91.47404276 89.97151355]]\n",
      "[[91.53267035 90.02918301]]\n",
      "[[91.59123693 90.08679245]]\n",
      "[[91.64974263 90.14434201]]\n",
      "[[91.70818759 90.20183182]]\n",
      "The train loss is 289.2879404146775\n",
      "The val loss is 142.43975197084023\n",
      "[[91.76657196 90.25926203]]\n",
      "[[91.82489587 90.31663277]]\n",
      "[[91.88315945 90.37394417]]\n",
      "[[91.94136284 90.43119636]]\n",
      "[[91.99950618 90.48838949]]\n",
      "[[92.05758961 90.54552367]]\n",
      "[[92.11561325 90.60259906]]\n",
      "[[92.17357725 90.65961577]]\n",
      "[[92.23148173 90.71657394]]\n",
      "[[92.28932684 90.77347371]]\n",
      "The train loss is 290.1797091076985\n",
      "The val loss is 142.87796981019818\n",
      "[[92.34711269 90.83031519]]\n",
      "[[92.40483944 90.88709854]]\n",
      "[[92.46250721 90.94382386]]\n",
      "[[92.52011612 91.0004913 ]]\n",
      "[[92.57766632 91.05710098]]\n",
      "[[92.63515793 91.11365303]]\n",
      "[[92.69259109 91.17014758]]\n",
      "[[92.74996591 91.22658475]]\n",
      "[[92.80728255 91.28296468]]\n",
      "[[92.86454111 91.3392875 ]]\n",
      "The train loss is 291.05992800226534\n",
      "The val loss is 143.31052106523995\n",
      "[[92.92174173 91.39555332]]\n",
      "[[92.97888455 91.45176227]]\n",
      "[[93.03596968 91.50791448]]\n",
      "[[93.09299725 91.56401008]]\n",
      "[[93.14996739 91.62004919]]\n",
      "[[93.20688024 91.67603194]]\n",
      "[[93.2637359  91.73195844]]\n",
      "[[93.32053452 91.78782882]]\n",
      "[[93.3772762  91.84364321]]\n",
      "[[93.43396109 91.89940173]]\n",
      "The train loss is 291.9288901747908\n",
      "The val loss is 143.73754929794262\n",
      "[[93.4905893  91.95510449]]\n",
      "[[93.54716096 92.01075163]]\n",
      "[[93.60367619 92.06634326]]\n",
      "[[93.66013511 92.12187951]]\n",
      "[[93.71653785 92.17736049]]\n",
      "[[93.77288453 92.23278633]]\n",
      "[[93.82917527 92.28815714]]\n",
      "[[93.88541019 92.34347304]]\n",
      "[[93.94158942 92.39873416]]\n",
      "[[93.99771306 92.45394062]]\n",
      "The train loss is 292.7868777478221\n",
      "The val loss is 144.15919271308877\n",
      "[[94.05378125 92.50909252]]\n",
      "[[94.10979411 92.56418999]]\n",
      "[[94.16575175 92.61923315]]\n",
      "[[94.22165429 92.67422211]]\n",
      "[[94.27750185 92.72915698]]\n",
      "[[94.33329455 92.7840379 ]]\n",
      "[[94.38903251 92.83886497]]\n",
      "[[94.44471584 92.8936383 ]]\n",
      "[[94.50034466 92.94835802]]\n",
      "[[94.55591909 93.00302423]]\n",
      "The train loss is 293.63416242672713\n",
      "The val loss is 144.5755844203253\n",
      "[[94.61143924 93.05763706]]\n",
      "[[94.66690524 93.11219661]]\n",
      "[[94.72231719 93.166703  ]]\n",
      "[[94.77767521 93.22115634]]\n",
      "[[94.83297942 93.27555675]]\n",
      "[[94.88822993 93.32990434]]\n",
      "[[94.94342685 93.38419921]]\n",
      "[[94.99857029 93.43844149]]\n",
      "[[95.05366038 93.49263128]]\n",
      "[[95.10869723 93.5467687 ]]\n",
      "The train loss is 294.4710060040407\n",
      "The val loss is 144.98685268045756\n",
      "[[95.16368093 93.60085385]]\n",
      "[[95.21861162 93.65488684]]\n",
      "[[95.2734894  93.70886779]]\n",
      "[[95.32831438 93.76279681]]\n",
      "[[95.38308667 93.816674  ]]\n",
      "[[95.43780638 93.87049947]]\n",
      "[[95.49247363 93.92427333]]\n",
      "[[95.54708853 93.9779957 ]]\n",
      "[[95.60165117 94.03166667]]\n",
      "[[95.65616168 94.08528635]]\n",
      "The train loss is 295.29766083377416\n",
      "The val loss is 145.393121137096\n",
      "[[95.71062016 94.13885486]]\n",
      "[[95.76502672 94.1923723 ]]\n",
      "[[95.81938147 94.24583877]]\n",
      "[[95.87368451 94.29925438]]\n",
      "[[95.92793596 94.35261924]]\n",
      "[[95.98213592 94.40593345]]\n",
      "[[96.03628449 94.45919712]]\n",
      "[[96.09038179 94.51241036]]\n",
      "[[96.14442792 94.56557326]]\n",
      "[[96.19842299 94.61868593]]\n",
      "The train loss is 296.1143702777946\n",
      "The val loss is 145.79450903468708\n",
      "[[96.25236709 94.67174847]]\n",
      "[[96.30626035 94.724761  ]]\n",
      "[[96.36010285 94.7777236 ]]\n",
      "[[96.41389472 94.83063639]]\n",
      "[[96.46763604 94.88349946]]\n",
      "[[96.52132692 94.93631292]]\n",
      "[[96.57496748 94.98907688]]\n",
      "[[96.6285578  95.04179142]]\n",
      "[[96.682098   95.09445666]]\n",
      "[[96.73558817 95.14707269]]\n",
      "The train loss is 296.9213691262293\n",
      "The val loss is 146.1911314238734\n",
      "[[96.78902842 95.19963961]]\n",
      "[[96.84241885 95.25215753]]\n",
      "[[96.89575956 95.30462655]]\n",
      "[[96.94905065 95.35704675]]\n",
      "[[97.00229223 95.40941825]]\n",
      "[[97.05548439 95.46174114]]\n",
      "[[97.10862723 95.51401553]]\n",
      "[[97.16172086 95.5662415 ]]\n",
      "[[97.21476538 95.61841915]]\n",
      "[[97.26776087 95.6705486 ]]\n",
      "The train loss is 297.718883993674\n",
      "The val loss is 146.5830993550519\n",
      "[[97.32070745 95.72262992]]\n",
      "[[97.37360521 95.77466322]]\n",
      "[[97.42645425 95.8266486 ]]\n",
      "[[97.47925466 95.87858615]]\n",
      "[[97.53200655 95.93047597]]\n",
      "[[97.58471002 95.98231815]]\n",
      "[[97.63736515 96.0341128 ]]\n",
      "[[97.68997205 96.08585999]]\n",
      "[[97.74253081 96.13755984]]\n",
      "[[97.79504153 96.18921243]]\n",
      "The train loss is 298.50713369285006\n",
      "The val loss is 146.97052006093514\n",
      "[[97.84750431 96.24081787]]\n",
      "[[97.89991924 96.29237623]]\n",
      "[[97.95228641 96.34388762]]\n",
      "[[98.00460593 96.39535214]]\n",
      "[[98.05687788 96.44676986]]\n",
      "[[98.10910236 96.4981409 ]]\n",
      "[[98.16127947 96.54946533]]\n",
      "[[98.2134093  96.60074325]]\n",
      "[[98.26549194 96.65197476]]\n",
      "[[98.31752748 96.70315995]]\n",
      "The train loss is 299.2863295872355\n",
      "The val loss is 147.3534971288517\n",
      "[[98.36951603 96.7542989 ]]\n",
      "[[98.42145766 96.80539171]]\n",
      "[[98.47335248 96.85643847]]\n",
      "[[98.52520058 96.90743927]]\n",
      "[[98.57700205 96.9583942 ]]\n",
      "[[98.62875697 97.00930335]]\n",
      "[[98.68046545 97.06016681]]\n",
      "[[98.73212757 97.11098468]]\n",
      "[[98.78374342 97.16175703]]\n",
      "[[98.8353131  97.21248396]]\n",
      "The train loss is 300.05667592406496\n",
      "The val loss is 147.73213066346958\n",
      "[[98.88683669 97.26316556]]\n",
      "[[98.93831428 97.31380192]]\n",
      "[[98.98974597 97.36439312]]\n",
      "[[99.04113184 97.41493925]]\n",
      "[[99.09247199 97.4654404 ]]\n",
      "[[99.14376649 97.51589666]]\n",
      "[[99.19501545 97.56630812]]\n",
      "[[99.24621895 97.61667485]]\n",
      "[[99.29737707 97.66699696]]\n",
      "[[99.3484899  97.71727452]]\n",
      "The train loss is 300.81837014898923\n",
      "The val loss is 148.1065174405714\n",
      "[[99.39955754 97.76750762]]\n",
      "[[99.45058007 97.81769635]]\n",
      "[[99.50155757 97.86784079]]\n",
      "[[99.55249014 97.91794102]]\n",
      "[[99.60337786 97.96799715]]\n",
      "[[99.65422081 98.01800924]]\n",
      "[[99.70501909 98.06797738]]\n",
      "[[99.75577277 98.11790166]]\n",
      "[[99.80648195 98.16778216]]\n",
      "[[99.85714671 98.21761896]]\n",
      "The train loss is 301.5716032035978\n",
      "The val loss is 148.47675105246358\n",
      "[[99.90776713 98.26741216]]\n",
      "[[99.9583433  98.31716182]]\n",
      "[[100.0088753   98.36686804]]\n",
      "[[100.05936322  98.4165309 ]]\n",
      "[[100.10980714  98.46615048]]\n",
      "[[100.16020715  98.51572687]]\n",
      "[[100.21056332  98.56526013]]\n",
      "[[100.26087575  98.61475037]]\n",
      "[[100.31114451  98.66419765]]\n",
      "[[100.36136969  98.71360207]]\n",
      "The train loss is 302.3165598068952\n",
      "The val loss is 148.8429220455596\n",
      "[[100.41155137  98.7629637 ]]\n",
      "[[100.46168964  98.81228262]]\n",
      "[[100.51178457  98.86155891]]\n",
      "[[100.56183624  98.91079265]]\n",
      "[[100.61184475  98.95998393]]\n",
      "[[100.66181016  99.00913283]]\n",
      "[[100.71173256  99.05823942]]\n",
      "[[100.76161204  99.10730378]]\n",
      "[[100.81144867  99.15632599]]\n",
      "[[100.86124253  99.20530614]]\n",
      "The train loss is 303.0534187217724\n",
      "The val loss is 149.20511805063242\n",
      "[[100.9109937  99.2542443]]\n",
      "[[100.96070227  99.30314054]]\n",
      "[[101.01036831  99.35199496]]\n",
      "[[101.0599919   99.40080762]]\n",
      "[[101.10957312  99.4495786 ]]\n",
      "[[101.15911205  99.49830799]]\n",
      "[[101.20860877  99.54699585]]\n",
      "[[101.25806336  99.59564227]]\n",
      "[[101.3074759   99.64424733]]\n",
      "[[101.35684646  99.69281109]]\n",
      "The train loss is 303.7823530074038\n",
      "The val loss is 149.5634239062036\n",
      "[[101.40617512  99.74133365]]\n",
      "[[101.45546196  99.78981506]]\n",
      "[[101.50470706  99.83825542]]\n",
      "[[101.5539105   99.88665479]]\n",
      "[[101.60307234  99.93501325]]\n",
      "[[101.65219267  99.98333088]]\n",
      "[[101.70127157 100.03160775]]\n",
      "[[101.75030911 100.07984394]]\n",
      "[[101.79930537 100.12803952]]\n",
      "[[101.84826042 100.17619457]]\n",
      "The train loss is 304.50353025846334\n",
      "The val loss is 149.91792177549124\n",
      "[[101.89717434 100.22430916]]\n",
      "[[101.9460472  100.27238336]]\n",
      "[[101.99487908 100.32041725]]\n",
      "[[102.04367006 100.36841091]]\n",
      "[[102.09242021 100.41636441]]\n",
      "[[102.1411296  100.46427781]]\n",
      "[[102.18979831 100.5121512 ]]\n",
      "[[102.23842641 100.55998465]]\n",
      "[[102.28701398 100.60777822]]\n",
      "[[102.33556109 100.655532  ]]\n",
      "The train loss is 305.2171128319639\n",
      "The val loss is 150.26869125732046\n",
      "[[102.38406782 100.70324605]]\n",
      "[[102.43253423 100.75092045]]\n",
      "[[102.4809604  100.79855527]]\n",
      "[[102.52934641 100.84615057]]\n",
      "[[102.57769233 100.89370644]]\n",
      "[[102.62599822 100.94122295]]\n",
      "[[102.67426416 100.98870015]]\n",
      "[[102.72249023 101.03613814]]\n",
      "[[102.7706765  101.08353697]]\n"
     ]
    }
   ],
   "source": [
    "nueralnet=MultiLayerNueralNet(x_train.shape[0],1,1)\n",
    "nueralnet.forward_propgation(x_train,y_train)\n",
    "nueralnet.backward_propagation(x_train,y_train)\n",
    "train_loss,test_loss=nueralnet.train(x_train,y_train,x_test,y_test,1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.plot(train_loss)\n",
    "plt.plot(test_loss)\n",
    "plt.ylabel('loss')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
